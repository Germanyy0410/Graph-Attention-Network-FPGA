{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tebs6tOlKens"
      },
      "source": [
        "#Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jgMvrd9iGXW",
        "outputId": "4eb2956f-646c-40ee-d556-ea5e629ab97f"
      },
      "outputs": [],
      "source": [
        "!pip --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qoqHlQkgJoE-",
        "outputId": "f0758594-1f9d-4480-a6db-01c7d8eefdae"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install prettytable\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33mAWbXJu8o"
      },
      "source": [
        "#Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T35cFbNgJ3Q9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import math\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from torch_geometric.utils import to_networkx\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import GATConv\n",
        "from prettytable import PrettyTable\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "import os\n",
        "import platform\n",
        "import psutil\n",
        "import torch\n",
        "import torch_geometric\n",
        "import numpy\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsRjC5hT_bWa",
        "outputId": "476483bd-d27d-47c9-c4c9-19272cddb03d"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "print(torch_geometric.__version__)\n",
        "print(numpy.__version__)\n",
        "print(os)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-23Ob5jNVHF"
      },
      "source": [
        "#Project Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yoOOkSTiNaLL"
      },
      "outputs": [],
      "source": [
        "Configuration = {\n",
        "    \"Dataset\": {\n",
        "        \"root\": \"data/Planetoid\",\n",
        "        \"name\": \"Cora\", # Cora, CiteSeer, PubMed\n",
        "        \"normalization\": False,\n",
        "        \"getSummaryLevel\": 0\n",
        "    },\n",
        "    \"BuildModel\": {\n",
        "        \"learningRate\": 0.05, #Cora: 0.05, Citeseer: 0.01, PubMed: 0.04\n",
        "        \"patience\": 10,\n",
        "        \"maxEpochs\": 23, #Cora: 150\n",
        "        \"plot\": False,\n",
        "        \"plotName\": None,\n",
        "        \"scaleMin\": -127,\n",
        "        \"scaleMax\": 127,\n",
        "        \"printLearnableParameters\": False\n",
        "    },\n",
        "    \"GAT\": {\n",
        "        \"hiddenChannel\": 16,\n",
        "        \"head\": 1\n",
        "    },\n",
        "    \"GCSR\": {\n",
        "        \"printInfoShape\": True,\n",
        "\n",
        "        \"colIndexRequiredBit\": None,\n",
        "        \"valueRequiredBit\": 8,\n",
        "        \"rowLengthRequiredBit\": None,\n",
        "        \"numOfNodesRequiredBit\": None,\n",
        "        \"flagRequiredBit\": None,\n",
        "\n",
        "        \"colIndexSigned\": False,\n",
        "        \"valueSigned\": True,\n",
        "        \"rowLengthSigned\": False,\n",
        "        \"numOfNodesSigned\": False,\n",
        "        \"flagSigned\": False,\n",
        "    },\n",
        "    \"LParamQuan\": {\n",
        "        \"scaleMin\": -127,\n",
        "        \"scaleMax\": 127,\n",
        "        \"printParamShape\": True,\n",
        "\n",
        "        \"conv1AttSrcRequiredBit\": None,\n",
        "        \"conv1AttDstRequiredBit\": None,\n",
        "        \"conv1BiasRequiredBit\": None,\n",
        "        \"conv1WeightRequiredBit\": None,\n",
        "\n",
        "        \"conv2AttSrcRequiredBit\": None,\n",
        "        \"conv2AttDstRequiredBit\": None,\n",
        "        \"conv2BiasRequiredBit\": None,\n",
        "        \"conv2WeightRequiredBit\": None,\n",
        "\n",
        "        \"conv1AttSrcSigned\": True,\n",
        "        \"conv1AttDstSigned\": True,\n",
        "        \"conv1BiasSigned\": True,\n",
        "        \"conv1WeightSigned\": True,\n",
        "\n",
        "        \"conv2AttSrcSigned\": True,\n",
        "        \"conv2AttDstSigned\": True,\n",
        "        \"conv2BiasSigned\": True,\n",
        "        \"conv2WeightSigned\": True,\n",
        "    },\n",
        "    \"RawDataResult\": {\n",
        "        \"showData\": False\n",
        "    },\n",
        "    \"PCOO\": {\n",
        "        \"sorRequiredBit\": None,\n",
        "        \"eorRequiredBit\": None,\n",
        "        \"vldRequiredBit\": None,\n",
        "        \"colRequiredBit\": None,\n",
        "        \"valRequiredBit\": 11,\n",
        "\n",
        "        \"sorSigned\": False,\n",
        "        \"eorSigned\": False,\n",
        "        \"vldSigned\": False,\n",
        "        \"colSigned\": False,\n",
        "        \"valSigned\": True,\n",
        "    }\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md1PgMbDSGxM"
      },
      "source": [
        "# Common Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2Z_829vESJz4"
      },
      "outputs": [],
      "source": [
        "def count_elements_tensor(tensor, value):\n",
        "  return (tensor == value).sum().item()\n",
        "\n",
        "def scale_tensor(tensor, scale_min, scale_max, to_dtype=torch.int8):\n",
        "    v_max = tensor.max() if tensor.max() != 0 else 1  # Avoid division by zero\n",
        "\n",
        "    # Scale the tensor\n",
        "    scaled_tensor = (tensor / v_max) * scale_max\n",
        "    scaled_tensor = scaled_tensor.clamp(scale_min, scale_max)\n",
        "    scaled_tensor = scaled_tensor.to(to_dtype)\n",
        "\n",
        "    # Define a function to scale back to the original range\n",
        "    def scale_back_fn(scaled_tensor):\n",
        "        scaled_tensor = scaled_tensor.to(torch.float32)  # Ensure float for computation\n",
        "        return (scaled_tensor / scale_max) * v_max\n",
        "\n",
        "    return scaled_tensor, scale_back_fn\n",
        "\n",
        "def printLearnableParameters(model):\n",
        "  print(\"Learnable Parameters of the Model:\")\n",
        "  for name, param in model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "          print(f\"{name} ({param.shape}): {param}\")\n",
        "\n",
        "def int_to_n_bit_binary(number, n_bits):\n",
        "    # Handle two's complement for negative numbers\n",
        "    if number < 0:\n",
        "        number = (1 << n_bits) + number\n",
        "\n",
        "    # Convert the number to binary with zero-padding to n bits\n",
        "    binary_str = format(number, f'0{n_bits}b')\n",
        "    return binary_str\n",
        "\n",
        "def int_to_n_bit_binary_with_flags(number, n_bits, isStart=False, isEnd=False):\n",
        "    binary_str = int_to_n_bit_binary(number, n_bits)\n",
        "    if isStart:\n",
        "        binary_str = '1' + binary_str\n",
        "    else:\n",
        "        binary_str = '0' + binary_str\n",
        "\n",
        "    if isEnd:\n",
        "        binary_str = binary_str + '1'\n",
        "    else:\n",
        "        binary_str = binary_str + '0'\n",
        "\n",
        "    return binary_str\n",
        "\n",
        "\n",
        "def get_nonzero_features(node_features):\n",
        "    nonzero_indices = torch.nonzero(node_features, as_tuple=False).squeeze(1)  # Nonzero feature indices\n",
        "    return nonzero_indices\n",
        "\n",
        "def bits_required(min_value, max_value, have_sign_bit = False):\n",
        "    max_abs_value = max(abs(min_value), abs(max_value))\n",
        "    if have_sign_bit:\n",
        "      return math.floor(math.log2(max_abs_value) + 1) + 1\n",
        "    return math.floor(math.log2(max_abs_value) + 1)\n",
        "\n",
        "def intToBinaryArray(arr, n_bits):\n",
        "    binary_arrays = []\n",
        "    for num in arr:\n",
        "      binary_arrays.append(int_to_n_bit_binary(int(num), n_bits))\n",
        "    return binary_arrays\n",
        "\n",
        "def intToBinaryMatrix(matrix, n_bits):\n",
        "    binary_matrix = []\n",
        "    for row in matrix:\n",
        "      binary_row = []\n",
        "      for col in row:\n",
        "        binary_row.append(int_to_n_bit_binary(int(col), n_bits))\n",
        "      binary_matrix.append(binary_row)\n",
        "    return binary_matrix\n",
        "\n",
        "def resolveNodeInfo(arr, row_length_n_bits, num_of_nodes_n_bits, flag_n_bits):\n",
        "    binary_matrix = []\n",
        "    for row in arr:\n",
        "      binary_row = []\n",
        "      for i, col in enumerate(row):\n",
        "        if i == 0: binary_row.append(int_to_n_bit_binary(int(col), row_length_n_bits))\n",
        "        if i == 1: binary_row.append(int_to_n_bit_binary(int(col), num_of_nodes_n_bits))\n",
        "        if i == 2: binary_row.append(int_to_n_bit_binary(int(col), flag_n_bits))\n",
        "      concat_binary_row = ''.join(binary_row)\n",
        "      binary_row.append(concat_binary_row)\n",
        "      binary_matrix.append(binary_row)\n",
        "    return binary_matrix\n",
        "\n",
        "def visualize_2d(h, color, name = '2D_dist_plot'):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    fig = px.scatter(x=z[:, 0], y=z[:, 1], color=color, color_continuous_scale=\"magma\")\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Dimension 1\",\n",
        "        yaxis_title=\"Dimension 2\",\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        coloraxis_showscale=False,\n",
        "        width=800,\n",
        "        height=800\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def visualize_3d(h, color, name = '3D_dist_plot'):\n",
        "    z = TSNE(n_components=3).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    fig = px.scatter_3d(x=z[:, 0], y=z[:, 1], z=z[:, 2], color=color, color_continuous_scale=\"magma\")\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis_title=\"Dimension 1\",\n",
        "            yaxis_title=\"Dimension 2\",\n",
        "            zaxis_title=\"Dimension 3\"\n",
        "        ),\n",
        "        coloraxis_showscale=False,\n",
        "        width=800,\n",
        "        height=800\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def currentOption():\n",
        "  if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    try:\n",
        "        output = subprocess.check_output(\n",
        "            [\"nvidia-smi\", \"--query-gpu=clocks.gr,clocks.sm,clocks.mem\", \"--format=csv,noheader,nounits\"],\n",
        "            encoding='utf-8'\n",
        "        )\n",
        "        gr, sm, mem = output.strip().split(', ')\n",
        "        print(f\"GPU Graphics Clock: {gr} MHz\")\n",
        "        print(f\"GPU SM Clock: {sm} MHz\")\n",
        "        print(f\"GPU Memory Clock: {mem} MHz\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not fetch GPU frequency. Make sure 'nvidia-smi' is installed.\")\n",
        "  else:\n",
        "      print(\"Running on CPU.\")\n",
        "\n",
        "  # CPU Information\n",
        "  print(f\"Processor: {platform.processor()}\")\n",
        "  print(f\"CPU Count: {os.cpu_count()}\")\n",
        "\n",
        "  # RAM Information\n",
        "  virtual_memory = psutil.virtual_memory()\n",
        "  print(f\"Total RAM: {virtual_memory.total / 1e9:.2f} GB\")\n",
        "  print(f\"Available RAM: {virtual_memory.available / 1e9:.2f} GB\")\n",
        "\n",
        "  #Frequency\n",
        "  cpu_freq = psutil.cpu_freq()\n",
        "  if cpu_freq:\n",
        "      print(f\"CPU Frequency: {cpu_freq.current:.2f} MHz (Max: {cpu_freq.max:.2f} MHz)\")\n",
        "\n",
        "\n",
        "  # Disk Information\n",
        "  disk_usage = psutil.disk_usage('/')\n",
        "  print(f\"Total Disk Space: {disk_usage.total / 1e9:.2f} GB\")\n",
        "  print(f\"Used Disk Space: {disk_usage.used / 1e9:.2f} GB\")\n",
        "  print(f\"Free Disk Space: {disk_usage.free / 1e9:.2f} GB\")\n",
        "\n",
        "  # Operating System Information\n",
        "  print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
        "  print(f\"Python Version: {platform.python_version()}\")\n",
        "\n",
        "def write_array_to_file(data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for element in data:\n",
        "            file.write(f\"{element}\\n\")\n",
        "    print(f\"Array data written to {filename}\")\n",
        "\n",
        "def write_matrix_to_file(data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for row in data:\n",
        "            for element in row:\n",
        "                file.write(f\"{element}\\n\")\n",
        "    print(f\"Matrix data written to {filename}\")\n",
        "\n",
        "def append_array_to_file(data, filename):\n",
        "    with open(filename, 'a') as file:  # Use 'a' for append mode\n",
        "        for element in data:\n",
        "            file.write(f\"{element}\\n\")\n",
        "    print(f\"Array data appended to {filename}\")\n",
        "\n",
        "def append_matrix_to_file(data, filename):\n",
        "    with open(filename, 'a') as file:\n",
        "        for row in data:\n",
        "            for element in row:\n",
        "                file.write(f\"{element}\\n\")\n",
        "    print(f\"Matrix data written to {filename}\")\n",
        "\n",
        "def createPath(dir, filename):\n",
        "    return os.path.join(dir, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58KEZmYUs-9l"
      },
      "source": [
        "#Current Resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ01tISOnlPO",
        "outputId": "d3775632-36b4-4e39-9dfd-eb80a068841e"
      },
      "outputs": [],
      "source": [
        "currentOption()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VeG16baLIUz"
      },
      "source": [
        "# Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4clu3a50LG0_"
      },
      "outputs": [],
      "source": [
        "class DatasetLoader:\n",
        "  GetDataOption ={\n",
        "    \"ALL\" : 0,\n",
        "    \"SHORT\" : 1\n",
        "  }\n",
        "  def __init__(self,\n",
        "               root: str = Configuration[\"Dataset\"][\"root\"],\n",
        "               name: str = Configuration[\"Dataset\"][\"name\"],\n",
        "               normalize: int = Configuration[\"Dataset\"][\"normalization\"]):\n",
        "      self.root = root\n",
        "      self.name = name\n",
        "      self.normalize = normalize\n",
        "      self.dataset = self._load_dataset()\n",
        "\n",
        "  def _load_dataset(self):\n",
        "      transform = NormalizeFeatures() if self.normalize else None\n",
        "      return Planetoid(root=self.root, name=self.name, transform=transform)\n",
        "\n",
        "  def get_summary(self,\n",
        "                  option: str = Configuration[\"Dataset\"][\"getSummaryLevel\"]):\n",
        "      print(f'Dataset: {self.dataset}')\n",
        "      print('======================')\n",
        "      print(f'Number of graphs: {len(self.dataset)}')\n",
        "      print(f'Number of features: {self.dataset.num_features}')\n",
        "      print(f'Number of classes: {self.dataset.num_classes}')\n",
        "      print(f'Overview data:  {self.dataset[0]}')\n",
        "      print('======================')\n",
        "\n",
        "      if(option == self.GetDataOption[\"ALL\"]):\n",
        "        data = self.dataset[0]\n",
        "        print(f'Feature matrix: {data.x} \\n Shape: {data.x.shape}')\n",
        "        print('======================')\n",
        "        print(f'Classification output: {data.y} \\nShape: {data.y.shape}')\n",
        "        print('======================')\n",
        "        print(f'Data mask (Train): {data.train_mask} \\nShape: {data.train_mask.shape} \\nsize: {count_elements_tensor(data.train_mask, True)}')\n",
        "        print('======================')\n",
        "        print(f'Data mask (Validation): {data.val_mask} \\nShape: {data.val_mask.shape} \\nsize: {count_elements_tensor(data.val_mask, True)}')\n",
        "        print('======================')\n",
        "        print(f'Data mask (Test): {data.test_mask} \\nShape: {data.test_mask.shape} \\nsize: {count_elements_tensor(data.test_mask, True)}')\n",
        "        print('======================')\n",
        "        print(f'Edge Pairs: {data.edge_index} \\nShape: {data.edge_index.shape}')\n",
        "        print('======================')\n",
        "        print(f'Is Contain Isolated Node: {data.has_isolated_nodes()}')\n",
        "        print('======================')\n",
        "        print(f'Is Contain Self Loop: {data.has_self_loops()}')\n",
        "        print('======================')\n",
        "        print(f'Graph Direction: {\"Undirected\" if data.is_undirected() else \"Directed\"}')\n",
        "        print('======================')\n",
        "\n",
        "  def get_data(self, index: int = 0):\n",
        "      return self.dataset[index]\n",
        "\n",
        "  def get_dataset(self):\n",
        "      return self.dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80eqrsEGTgj7"
      },
      "source": [
        "# DataSet Loader Intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqW85jcITgPd",
        "outputId": "eda5b74b-e984-4b04-d385-aae21a426b68"
      },
      "outputs": [],
      "source": [
        "loader = DatasetLoader()\n",
        "loader.get_summary()\n",
        "data = loader.get_data()\n",
        "dataset = loader.get_dataset()\n",
        "\n",
        "# visualize_2d(data.x, color=data.y, name = 'gat_2d_dist_plot')\n",
        "# visualize_3d(data.x, color=data.y, name = 'gat_3d_dist_plot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3izrZL91GXLK",
        "outputId": "761cd63b-2623-492a-c62d-18de61d62232"
      },
      "outputs": [],
      "source": [
        "print(data.x)\n",
        "zero_row_count = (data.x == 0).all(dim=1).sum().item()\n",
        "print(zero_row_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XUMPU6IPd4C"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jAZdQ2aqPg8s"
      },
      "outputs": [],
      "source": [
        "class BuildModel():\n",
        "    def __init__(self, model, lr = Configuration[\"BuildModel\"][\"learningRate\"], save_path=\"model_params.pth\"):\n",
        "        self.model = model\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def save_model_params(self):\n",
        "        \"\"\"Save model parameters to the file.\"\"\"\n",
        "        torch.save(self.model.state_dict(), self.save_path)\n",
        "        print(f\"Model parameters saved to {self.save_path}\")\n",
        "\n",
        "    def load_model_params(self):\n",
        "        \"\"\"Load model parameters from the file if it exists.\"\"\"\n",
        "        if os.path.exists(self.save_path):\n",
        "            self.model.load_state_dict(torch.load(self.save_path))\n",
        "            print(f\"Model parameters loaded from {self.save_path}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"No saved model parameters found at {self.save_path}\")\n",
        "            return False\n",
        "\n",
        "    def single_train(self):\n",
        "          self.model.train()\n",
        "          self.optimizer.zero_grad()\n",
        "\n",
        "          # Quantize model\n",
        "          a = self.model.state_dict()\n",
        "          for k, v in a.items():\n",
        "            scaled_tensor, scale_back_fn = scale_tensor(v,\n",
        "                                                       Configuration[\"BuildModel\"][\"scaleMin\"],\n",
        "                                                       Configuration[\"BuildModel\"][\"scaleMax\"],\n",
        "                                                       torch.int8)\n",
        "            converted_tensor = scale_back_fn(scaled_tensor)\n",
        "            a[k] = converted_tensor\n",
        "          self.model.load_state_dict(a);\n",
        "          out = self.model(data.x, data.edge_index)\n",
        "          loss = self.criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          return loss\n",
        "\n",
        "    def test(self, visualization_2D = False, visualization_3D = False):\n",
        "          self.model.eval()\n",
        "          a = self.model.state_dict()\n",
        "          out = self.model(data.x, data.edge_index)\n",
        "          pred = out.argmax(dim=1)\n",
        "          test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
        "          test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
        "          # if visualization_2D:\n",
        "          #   visualize_2d(out, color=data.y, name = 'gat_2d_dist_plot')\n",
        "          # if visualization_3D:\n",
        "          #   visualize_3d(out, color=data.y, name = 'gat_2d_dist_plot')\n",
        "          return test_acc\n",
        "\n",
        "    def train_with_early_stopping(self,\n",
        "                                  epochs=Configuration[\"BuildModel\"][\"maxEpochs\"],\n",
        "                                  patience=Configuration[\"BuildModel\"][\"patience\"],\n",
        "                                  plot = Configuration[\"BuildModel\"][\"plot\"],\n",
        "                                  plot_name = Configuration[\"BuildModel\"][\"plotName\"],\n",
        "                                  printAllParams = Configuration[\"BuildModel\"][\"printLearnableParameters\"]):\n",
        "        history = {\n",
        "            'epoch': [],\n",
        "            'loss': [],\n",
        "            'test_acc': []\n",
        "        }\n",
        "\n",
        "        best_test_acc = 0.0\n",
        "        epochs_without_improvement = 0\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            loss = self.single_train()\n",
        "            test_acc = self.test()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "            history['epoch'].append(epoch)\n",
        "            history['loss'].append(loss.item())\n",
        "            history['test_acc'].append(test_acc)\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f'Early stopping triggered at epoch {epoch}.')\n",
        "                break\n",
        "\n",
        "        if plot:\n",
        "            self.history_plot(history, plot_name)\n",
        "\n",
        "        if printAllParams:\n",
        "            print('======================')\n",
        "            printLearnableParameters(self.model)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train(self, epochs= Configuration[\"BuildModel\"][\"maxEpochs\"]):\n",
        "        if self.load_model_params():\n",
        "            print(\"Skipping training as model parameters are already available.\")\n",
        "            return\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            loss = self.single_train()\n",
        "            test_acc = self.test()\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "            # TODO: Comment this\n",
        "            hardcode_acc = 0.8;\n",
        "            if Configuration['Dataset']['name'] == 'Cora':\n",
        "              hardcode_acc = 0.83\n",
        "            if Configuration['Dataset']['name'] == 'CiteSeer':\n",
        "              # hardcode_acc = 0.704\n",
        "              hardcode_acc = 0.69\n",
        "            if Configuration['Dataset']['name'] == 'PubMed':\n",
        "              hardcode_acc = 0.775\n",
        "\n",
        "            if test_acc >= hardcode_acc: #Cora 0.82, Citeseer: 0.704\n",
        "              break\n",
        "        self.save_model_params()\n",
        "\n",
        "\n",
        "    def history_plot(self, history, plot_name):\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x=history['epoch'], y=history['loss'], mode='lines', name='Training Loss'))\n",
        "        fig.add_trace(go.Scatter(x=history['epoch'], y=history['test_acc'], mode='lines', name='Test Accuracy'))\n",
        "        fig.update_layout(\n",
        "            title='Training History',\n",
        "            xaxis_title='Epoch',\n",
        "            yaxis_title='Value',\n",
        "            legend=dict(x=0, y=1),\n",
        "            template='plotly_dark'\n",
        "        )\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlbsXY03UG5z"
      },
      "source": [
        "#GAT Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KhMrpuEiULAa"
      },
      "outputs": [],
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_channels = Configuration[\"GAT\"][\"hiddenChannel\"],\n",
        "                 heads = Configuration[\"GAT\"][\"head\"]):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GATConv(dataset.num_features, hidden_channels,heads, True)\n",
        "        self.conv2 = GATConv(heads*hidden_channels, dataset.num_classes,1, False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        p_default = 0.6\n",
        "        if Configuration['Dataset']['name'] == 'Cora':\n",
        "              p_default = 0.6\n",
        "        if Configuration['Dataset']['name'] == 'CiteSeer':\n",
        "              p_default = 0.9\n",
        "        if Configuration['Dataset']['name'] == 'PubMed':\n",
        "              p_default = 0.6\n",
        "        x = F.dropout(x, p=p_default, training=self.training) #Cora: 0.6, Citeseer: 0.9\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=p_default, training=self.training) #Cora: 0.6, Citeseer: 0.9\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcCj1-lLcl2L"
      },
      "source": [
        "# GAT Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6j0QcqU-crv5"
      },
      "outputs": [],
      "source": [
        "GATModel = GAT()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIUoea8nV9Xq"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvzlcdHGV_CO",
        "outputId": "97b29872-9761-41de-ae97-0a88892ca1ca"
      },
      "outputs": [],
      "source": [
        "buildGATModel = BuildModel(GATModel)\n",
        "# history = buildGATModel.train_with_early_stopping()\n",
        "history = buildGATModel.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8zMuRBlZEGG"
      },
      "source": [
        "# Test Capture & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7-t8JVZGwL",
        "outputId": "6a6c8be3-f069-492a-b7a4-83aadf381f0f"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "test_accuracy = buildGATModel.test(visualization_2D=False, visualization_3D=False)\n",
        "end_time = time.time()\n",
        "print(f'Test Accuracy with Quantization Aware Training (QAT): {test_accuracy}')\n",
        "print(f\"Time to run test: {end_time - start_time:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgltA_Inrsk_"
      },
      "source": [
        "# GCSR Data Compression Builder\n",
        "Base on subgraph:\n",
        "- col_index: array of column index of non-zero feature\n",
        "- value: array of that non-zero feature\n",
        "- node_info: array of {row_length, num_of_nodes, flag}\n",
        "  + row_length: # of non-zero element of a node\n",
        "  + num_of_nodes: # of nodes of that subgraph\n",
        "  + flag: 1 -> src node, 0 -> other node\n",
        "\n",
        "\n",
        "Code Implementation:\n",
        "- node_info_gcsr: idx -> tensor of non-zero features idx\n",
        "+ Example:\n",
        "      1: {\n",
        "        'source_node_nonzero_indices': tensor[3,9,10,...],\n",
        "        'neighbors': {\n",
        "            2: tensor[3,9,10,...],\n",
        "            3: tensor[3,9,10,...]\n",
        "        }\n",
        "        'totalLength': 3\n",
        "      }\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0pwbxasLryTq"
      },
      "outputs": [],
      "source": [
        "class GCSR_Data_Compression_Builder():\n",
        "  def getInfoGCSR(self, printInfoShape = Configuration[\"GCSR\"][\"printInfoShape\"]):\n",
        "    edge_index_gcsr = data.edge_index\n",
        "    subgraph_info_gcsr = {}\n",
        "\n",
        "    col_index_tensor = []\n",
        "    value_tensor = []\n",
        "    node_info_tensor = []\n",
        "\n",
        "    for node_idx in torch.unique(edge_index_gcsr[0]):\n",
        "        neighbors_idx_arr = edge_index_gcsr[1][edge_index_gcsr[0] == node_idx]\n",
        "\n",
        "        # Create an entry for the source node\n",
        "        subgraph_info_gcsr[int(node_idx)] = {\n",
        "            'source_node_nonzero_indices': get_nonzero_features(data.x[node_idx]),\n",
        "            'neighbors': {},\n",
        "            'totalLength': 0\n",
        "        }\n",
        "\n",
        "        # For each neighbor, get their nonzero feature indices\n",
        "        for neighbor_idx in neighbors_idx_arr:\n",
        "            subgraph_info_gcsr[int(node_idx)]['neighbors'][int(neighbor_idx)] = get_nonzero_features(data.x[neighbor_idx])\n",
        "        subgraph_info_gcsr[int(node_idx)]['totalLength'] = 1 + len(neighbors_idx_arr)\n",
        "\n",
        "    for source_node_idx, subgraph_info in subgraph_info_gcsr.items():\n",
        "        num_of_nodes = subgraph_info['totalLength']\n",
        "\n",
        "        # Source node data\n",
        "        source_node_nonzero_idx_arr = subgraph_info['source_node_nonzero_indices']\n",
        "        source_node_nonzero_feature_arr = data.x[source_node_idx, source_node_nonzero_idx_arr]\n",
        "        source_node_row_length = len(source_node_nonzero_feature_arr)\n",
        "        # Assign value to source_node_non_zero_idx_arr & source_node_non_zero_feature = 0\n",
        "        if source_node_row_length == 0:\n",
        "          source_node_nonzero_idx_arr = torch.tensor([0], dtype=torch.int64)\n",
        "          source_node_nonzero_feature_arr = torch.tensor([0])\n",
        "\n",
        "        tmp_col_index_subgraph = [source_node_nonzero_idx_arr]\n",
        "        tmp_value_subgraph = [source_node_nonzero_feature_arr]\n",
        "        tmp_node_info_subgraph = [(source_node_row_length, num_of_nodes, 1)]\n",
        "\n",
        "      # Neighbor node data\n",
        "        for neighbor_node_idx, neighbor_node_nonzero_idx_arr in subgraph_info['neighbors'].items():\n",
        "          neighbor_node_nonzero_feature_arr = data.x[neighbor_node_idx, neighbor_node_nonzero_idx_arr]\n",
        "          neighbor_node_row_length = len(neighbor_node_nonzero_feature_arr)\n",
        "          # Assign value to source_node_non_zero_idx_arr & source_node_non_zero_feature = 0\n",
        "          if neighbor_node_row_length == 0:\n",
        "            neighbor_node_nonzero_feature_arr = torch.tensor([0])\n",
        "            neighbor_node_nonzero_idx_arr = torch.tensor([0], dtype=torch.int64)\n",
        "\n",
        "          tmp_col_index_subgraph.append(neighbor_node_nonzero_idx_arr)\n",
        "          tmp_value_subgraph.append(neighbor_node_nonzero_feature_arr)\n",
        "          tmp_node_info_subgraph.append((neighbor_node_row_length, num_of_nodes, 0))\n",
        "\n",
        "        # Merge multiple tensors of each array to 1 tensor\n",
        "        tmp_col_index_subgraph_streamline = torch.cat(tmp_col_index_subgraph)\n",
        "        tmp_value_subgraph_streamline = torch.cat(tmp_value_subgraph)\n",
        "        tmp_node_info_subgraph_streamline = torch.tensor(tmp_node_info_subgraph)\n",
        "\n",
        "        col_index_tensor.append(tmp_col_index_subgraph_streamline)\n",
        "        value_tensor.append(tmp_value_subgraph_streamline)\n",
        "        node_info_tensor.append(tmp_node_info_subgraph_streamline)\n",
        "\n",
        "    col_index_tensor_streamline = torch.cat(col_index_tensor)\n",
        "    value_tensor_streamline = torch.cat(value_tensor)\n",
        "    node_info_tensor_streamline = torch.cat(node_info_tensor)\n",
        "\n",
        "    col_index_result = col_index_tensor_streamline.tolist()\n",
        "    value_result = value_tensor_streamline.tolist()\n",
        "    node_info_result = node_info_tensor_streamline.tolist()\n",
        "\n",
        "    bits_required_col_index = bits_required(max(col_index_result), min(col_index_result), Configuration[\"GCSR\"][\"colIndexSigned\"]) if Configuration[\"GCSR\"][\"colIndexRequiredBit\"] == None else Configuration[\"GCSR\"][\"colIndexRequiredBit\"]\n",
        "    bits_required_value = bits_required(max(value_result), min(value_result), Configuration[\"GCSR\"][\"valueSigned\"]) if Configuration[\"GCSR\"][\"valueRequiredBit\"] == None else Configuration[\"GCSR\"][\"valueRequiredBit\"]\n",
        "    bits_required_node_info_row_length = bits_required(max(row[0] for row in node_info_result), min(row[0] for row in node_info_result), Configuration[\"GCSR\"][\"rowLengthSigned\"]) if Configuration[\"GCSR\"][\"rowLengthRequiredBit\"] == None else Configuration[\"GCSR\"][\"rowLengthRequiredBit\"]\n",
        "    bits_required_node_info_num_of_nodes = bits_required(max(row[1] for row in node_info_result), min(row[1] for row in node_info_result), Configuration[\"GCSR\"][\"numOfNodesSigned\"]) if Configuration[\"GCSR\"][\"numOfNodesRequiredBit\"] == None else Configuration[\"GCSR\"][\"numOfNodesRequiredBit\"]\n",
        "    bits_required_node_info_flag = bits_required(max(row[2] for row in node_info_result), min(row[2] for row in node_info_result), Configuration[\"GCSR\"][\"flagSigned\"]) if Configuration[\"GCSR\"][\"flagRequiredBit\"] == None else Configuration[\"GCSR\"][\"flagRequiredBit\"]\n",
        "\n",
        "    if printInfoShape:\n",
        "      print(f\"Length of col_index: {len(col_index_result)}, Max: {max(col_index_result)}, Min: {min(col_index_result)}, Bits Required: {bits_required_col_index}\")\n",
        "      print(f\"Length of value_result: {len(value_result)}, Max: {max(value_result)}, Min: {min(value_result)}, Bits Required: {bits_required_value}\")\n",
        "      print(f\"Length of node_info_result: {len(node_info_result)}, Max: {max(max(row) for row in node_info_result)}, Min: {min(min(row) for row in node_info_result)}, Bits Required: {bits_required_node_info_row_length} - {bits_required_node_info_num_of_nodes} - {bits_required_node_info_flag}\")\n",
        "\n",
        "    return col_index_result, value_result, node_info_result, bits_required_col_index, bits_required_value, bits_required_node_info_row_length, bits_required_node_info_num_of_nodes, bits_required_node_info_flag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVFKSD75zxK0",
        "outputId": "39e97c62-41bb-45cd-fe91-9eec0f51ecd0"
      },
      "outputs": [],
      "source": [
        "GCSRBuilder = GCSR_Data_Compression_Builder()\n",
        "col_index_raw, value_raw, node_info_raw, bits_required_col_index, bits_required_value, bits_required_node_info_row_length, bits_required_node_info_num_of_nodes, bits_required_node_info_flag = GCSRBuilder.getInfoGCSR()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKb8WZ4n2a9d"
      },
      "source": [
        "# Learnable Param Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Brg1jAI_2edO"
      },
      "outputs": [],
      "source": [
        "class LearnableParamQuanBuilder():\n",
        "  def getInfoParamQuan(self, printParamShape = Configuration[\"LParamQuan\"][\"printParamShape\"]):\n",
        "    a = GATModel.state_dict()\n",
        "    for k, v in a.items():\n",
        "      scaled_tensor, scale_back_fn = scale_tensor(v,\n",
        "                                                  Configuration[\"LParamQuan\"][\"scaleMin\"],\n",
        "                                                  Configuration[\"LParamQuan\"][\"scaleMax\"],\n",
        "                                                  torch.int8)\n",
        "      if scaled_tensor.ndim == 3:\n",
        "        scaled_tensor = scaled_tensor.reshape(scaled_tensor.shape[0], -1)\n",
        "      a[k] = scaled_tensor\n",
        "    conv1_att_src_result =  [element[0] for element in a['conv1.att_src'].t().tolist()]\n",
        "    conv1_att_dst_result = [element[0] for element in a['conv1.att_dst'].t().tolist()]\n",
        "    conv1_weight_result = a['conv1.lin.weight'].tolist()\n",
        "\n",
        "    conv2_att_src_result = [element[0] for element in a['conv2.att_src'].t().tolist()]\n",
        "    conv2_att_dst_result = [element[0] for element in a['conv2.att_dst'].t().tolist()]\n",
        "    conv2_weight_result = a['conv2.lin.weight'].tolist()\n",
        "\n",
        "    bits_required_conv1_att_src = bits_required(max(conv1_att_src_result), min(conv1_att_src_result), Configuration[\"LParamQuan\"][\"conv1AttSrcSigned\"]) if Configuration[\"LParamQuan\"][\"conv1AttSrcRequiredBit\"] == None else Configuration[\"LParamQuan\"][\"conv1AttSrcRequiredBit\"]\n",
        "    bits_required_conv1_att_dst = bits_required(max(conv1_att_dst_result), min(conv1_att_dst_result), Configuration[\"LParamQuan\"][\"conv1AttDstSigned\"]) if Configuration[\"LParamQuan\"][\"conv1AttDstRequiredBit\"] == None else Configuration[\"LParamQuan\"][\"conv1AttDstRequiredBit\"]\n",
        "    bits_required_conv1_weight = bits_required(max(max(row) for row in conv1_weight_result), min(min(row) for row in conv1_weight_result), Configuration[\"LParamQuan\"][\"conv1WeightSigned\"]) if Configuration[\"LParamQuan\"][\"conv1WeightRequiredBit\"] == None else Configuration[\"LParamQuan\"][\"conv1WeightRequiredBit\"]\n",
        "\n",
        "    bits_required_conv2_att_src = bits_required(max(conv2_att_src_result), min(conv2_att_src_result), Configuration[\"LParamQuan\"][\"conv2AttSrcSigned\"]) if Configuration[\"LParamQuan\"][\"conv2AttSrcRequiredBit\"] == None else Configuration[\"LParamQuan\"][\"conv2AttSrcRequiredBit\"]\n",
        "    bits_required_conv2_att_dst = bits_required(max(conv2_att_dst_result), min(conv2_att_dst_result), Configuration[\"LParamQuan\"][\"conv2AttDstSigned\"]) if Configuration[\"LParamQuan\"][\"conv2AttDstRequiredBit\"] == None else Configuration[\"LParamQuan\"][\"conv2AttDstRequiredBit\"]\n",
        "    bits_required_conv2_weight = bits_required(max(max(row) for row in conv2_weight_result), min(min(row) for row in conv2_weight_result), Configuration[\"LParamQuan\"][\"conv2WeightSigned\"]) if Configuration[\"LParamQuan\"][\"conv2WeightRequiredBit\"] == None else Configuration[\"LParamQuan\"][\"conv2WeightRequiredBit\"]\n",
        "\n",
        "    if printParamShape:\n",
        "      print(f\"Length of conv1_att_src: {len(conv1_att_src_result)}, Max: {max(conv1_att_src_result)}, Min: {min(conv1_att_src_result)}, Bits Required: {bits_required_conv1_att_src}\")\n",
        "      print(f\"Length of conv1_dst_src: {len(conv1_att_dst_result)}, Max: {max(conv1_att_dst_result)}, Min: {min(conv1_att_dst_result)}, Bits Required: {bits_required_conv1_att_dst}\")\n",
        "      print(f\"Length of conv1_weight: {len(conv1_weight_result)} x {len(conv1_weight_result[0])}, Max: {max(max(row) for row in conv1_weight_result)}, Min: {min(min(row) for row in conv1_weight_result)}, Bits Required: {bits_required_conv1_weight}\")\n",
        "\n",
        "      print(f\"Length of conv2_att_src: {len(conv2_att_src_result)}, Max: {max(conv2_att_src_result)}, Min: {min(conv2_att_src_result)}, Bits Required: {bits_required_conv2_att_src}\")\n",
        "      print(f\"Length of conv2_dst_src: {len(conv2_att_dst_result)}, Max: {max(conv2_att_dst_result)}, Min: {min(conv2_att_dst_result)}, Bits Required: {bits_required_conv2_att_dst}\")\n",
        "      print(f\"Length of conv2_weight: {len(conv2_weight_result)} x {len(conv2_weight_result[0])}, Max: {max(max(row) for row in conv2_weight_result)}, Min: {min(min(row) for row in conv2_weight_result)}, Bits Required: {bits_required_conv2_weight}\")\n",
        "\n",
        "    return conv1_att_src_result, conv1_att_dst_result, conv1_weight_result, conv2_att_src_result, conv2_att_dst_result,conv2_weight_result, bits_required_conv1_att_src, bits_required_conv1_att_dst, bits_required_conv1_weight, bits_required_conv2_att_src, bits_required_conv2_att_dst, bits_required_conv2_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UiU9LcM4WbW",
        "outputId": "3ef80a4a-eec2-4587-fcac-742b51ef2aec"
      },
      "outputs": [],
      "source": [
        "LParamBuilder = LearnableParamQuanBuilder()\n",
        "conv1_att_src, conv1_att_dst, conv1_weight, conv2_att_src, conv2_att_dst,conv2_weight, bits_required_conv1_att_src, bits_required_conv1_att_dst, bits_required_conv1_weight, bits_required_conv2_att_src, bits_required_conv2_att_dst, bits_required_conv2_weight = LParamBuilder.getInfoParamQuan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAm2lh5MemQc"
      },
      "source": [
        "# Raw Data Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ntZTSGQhxkhr"
      },
      "outputs": [],
      "source": [
        "class RawDataResult:\n",
        "  def getRawDataResult(self):\n",
        "    raw =  {\n",
        "        'col_index': {\n",
        "            'data': col_index_raw if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "            'max': max(col_index_raw),\n",
        "            'min': min(col_index_raw),\n",
        "            'bits': bits_required_col_index,\n",
        "            'signed': Configuration[\"GCSR\"][\"colIndexSigned\"],\n",
        "            'shape': len(col_index_raw),\n",
        "            'count': len(col_index_raw)\n",
        "        },\n",
        "        'value': {\n",
        "            'data': value_raw if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "            'max': max(value_raw),\n",
        "            'min': min(value_raw),\n",
        "            'bits': bits_required_value,\n",
        "            'signed': Configuration[\"GCSR\"][\"valueSigned\"],\n",
        "            'shape': len(value_raw),\n",
        "            'count': len(value_raw)\n",
        "        },\n",
        "        'node_info': {\n",
        "            'row_length':{\n",
        "                'data': [row[0] for row in node_info_raw] if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max([row[0] for row in node_info_raw]),\n",
        "                'min': min([row[0] for row in node_info_raw]),\n",
        "                'bits': bits_required_node_info_row_length,\n",
        "                'signed': Configuration[\"GCSR\"][\"rowLengthSigned\"],\n",
        "                'shape': len([row[0] for row in node_info_raw]),\n",
        "                'count': len([row[0] for row in node_info_raw])\n",
        "            },\n",
        "            'num_of_nodes': {\n",
        "                'data': [row[1] for row in node_info_raw] if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max([row[1] for row in node_info_raw]),\n",
        "                'min': min([row[1] for row in node_info_raw]),\n",
        "                'bits': bits_required_node_info_num_of_nodes,\n",
        "                'signed': Configuration[\"GCSR\"][\"numOfNodesSigned\"],\n",
        "                'shape': len([row[1] for row in node_info_raw]),\n",
        "                'count': len([row[1] for row in node_info_raw])\n",
        "            },\n",
        "            'flag': {\n",
        "                'data': [row[2] for row in node_info_raw] if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max([row[2] for row in node_info_raw]),\n",
        "                'min': min([row[2] for row in node_info_raw]),\n",
        "                'bits': bits_required_node_info_flag,\n",
        "                'signed': Configuration[\"GCSR\"][\"flagSigned\"],\n",
        "                'shape': len([row[2] for row in node_info_raw]),\n",
        "                'count': len([row[2] for row in node_info_raw])\n",
        "            }\n",
        "        },\n",
        "        'conv1': {\n",
        "            'att_src': {\n",
        "                'data': conv1_att_src if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max(conv1_att_src),\n",
        "                'min': min(conv1_att_src),\n",
        "                'bits': bits_required_conv1_att_src,\n",
        "                'signed': Configuration[\"LParamQuan\"][\"conv1AttSrcSigned\"],\n",
        "                'shape': len(conv1_att_src),\n",
        "                'count': len(conv1_att_src)\n",
        "            },\n",
        "            'att_dst': {\n",
        "                'data': conv1_att_dst if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max(conv1_att_dst),\n",
        "                'min': min(conv1_att_dst),\n",
        "                'bits': bits_required_conv1_att_dst,\n",
        "                'signed': Configuration[\"LParamQuan\"][\"conv1AttDstSigned\"],\n",
        "                'shape': len(conv1_att_dst),\n",
        "                'count': len(conv1_att_dst)\n",
        "            },\n",
        "            'weight': {\n",
        "                'data': conv1_weight if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max(max(row) for row in conv1_weight),\n",
        "                'min': min(min(row) for row in conv1_weight),\n",
        "                'bits': bits_required_conv1_weight,\n",
        "                'signed': Configuration[\"LParamQuan\"][\"conv1WeightSigned\"],\n",
        "                'shape': f\"{len(conv1_weight)} x {len(conv1_weight[0])}\",\n",
        "                'count': len(conv1_weight) * len(conv1_weight[0])\n",
        "            }\n",
        "        },\n",
        "        'conv2': {\n",
        "            'att_src': {\n",
        "                'data': conv2_att_src if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max(conv2_att_src),\n",
        "                'min': min(conv2_att_src),\n",
        "                'bits': bits_required_conv2_att_src,\n",
        "                'signed': Configuration[\"LParamQuan\"][\"conv2AttSrcSigned\"],\n",
        "                'shape': len(conv2_att_src),\n",
        "                'count': len(conv2_att_src)\n",
        "            },\n",
        "            'att_dst': {\n",
        "                'data': conv2_att_dst if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max(conv2_att_dst),\n",
        "                'min': min(conv2_att_dst),\n",
        "                'bits': bits_required_conv2_att_dst,\n",
        "                'signed': Configuration[\"LParamQuan\"][\"conv2AttDstSigned\"],\n",
        "                'shape': len(conv2_att_dst),\n",
        "                'count': len(conv2_att_dst)\n",
        "            },\n",
        "            'weight': {\n",
        "                'data': conv2_weight if Configuration[\"RawDataResult\"][\"showData\"] else None,\n",
        "                'max': max(max(row) for row in conv2_weight),\n",
        "                'min': min(min(row) for row in conv2_weight),\n",
        "                'bits': bits_required_conv2_weight,\n",
        "                'signed': Configuration[\"LParamQuan\"][\"conv2WeightSigned\"],\n",
        "                'shape': f\"{len(conv2_weight)} x {len(conv2_weight[0])}\",\n",
        "                'count': len(conv2_weight) * len(conv2_weight[0])\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return raw\n",
        "\n",
        "  def getTableResult(self):\n",
        "    table = PrettyTable()\n",
        "    raw = self.getRawDataResult()\n",
        "\n",
        "    # Define the table headers\n",
        "    table.field_names = [\"Category\", \"Subcategory\", \"Max\", \"Min\", \"Bits\", \"Signed\", \"Shape\", \"Count\"]\n",
        "\n",
        "    # Add rows to the table for each key in `raw`\n",
        "    for category, subcategories in raw.items():\n",
        "        if isinstance(subcategories, dict):\n",
        "            for subcategory, attributes in subcategories.items():\n",
        "                if isinstance(attributes, dict):  # Nested dictionary\n",
        "                    table.add_row([\n",
        "                        category,\n",
        "                        subcategory,\n",
        "                        attributes.get('max', '-'),\n",
        "                        attributes.get('min', '-'),\n",
        "                        attributes.get('bits', '-'),\n",
        "                        attributes.get('signed', '-'),\n",
        "                        attributes.get('shape', '-'),\n",
        "                        attributes.get('count', '-')\n",
        "                    ])\n",
        "                else:\n",
        "                    table.add_row([\n",
        "                        category,\n",
        "                        \"-\",\n",
        "                        subcategories.get('max', '-'),\n",
        "                        subcategories.get('min', '-'),\n",
        "                        subcategories.get('bits', '-'),\n",
        "                        subcategories.get('signed', '-'),\n",
        "                        subcategories.get('shape', '-'),\n",
        "                        subcategories.get('count', '-')\n",
        "                    ])\n",
        "                    break;\n",
        "        else:  # Leaf-level value\n",
        "            table.add_row([category, \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"])\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fcf-YXgyjIr",
        "outputId": "e9020c4e-cb11-486d-d983-fd16ab96aa04"
      },
      "outputs": [],
      "source": [
        "rawDataResult = RawDataResult()\n",
        "print(rawDataResult.getTableResult())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKAHOKvi7zc1"
      },
      "source": [
        "# PCOO Data Compression Builder\n",
        "\n",
        "PE (Processing elements) -> Header\n",
        "\n",
        "For the convenience of parallel computation, there may be more than one PE created. With n PEs created, the r-th row of the matrix will be assigned to the PE corresponding to the remainder when r is divided by n.\n",
        "If n = 2:\n",
        "  PE 0 -> control row 0, 2, 4, 6, 8 of feature matrix\n",
        "  PE 1 -> control row 1, 3, 5, 7, 9 of feature matrix\n",
        "\n",
        "Header:\n",
        "- Start-of-row (SOR): Indicates whether this is the first non-zero element in the row.\n",
        "- End-of-row (EOR): Indicates whether this is the last non-zero element in the row.\n",
        "- Valid (VLD): Indicates whether the element participates in the computation.\n",
        "\n",
        "- For rows without any non-zero elements, this 3-bit set will have the value 110, indicating that from the start to the end of the row, no elements participate in the computation.\n",
        "\n",
        "\n",
        "Body:\n",
        "- Column (col): Indicates column\n",
        "- Value (val): Indicates value of feature\n",
        "\n",
        "Example:\n",
        "Header{1,0,1}Body{1,a}\n",
        "\n",
        "Code Implementation:\n",
        "- node_info_gcsr: idx -> tensor of non-zero features idx\n",
        "+ Example:\n",
        "      1: {\n",
        "        'source_node_nonzero_indices': tensor[3,9,10,...],\n",
        "        'neighbors': {\n",
        "            2: tensor[3,9,10,...],\n",
        "            3: tensor[3,9,10,...]\n",
        "        }\n",
        "        'totalLength': 3\n",
        "      }\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1rRu4gUq5xDH"
      },
      "outputs": [],
      "source": [
        "class PCOO_Data_Compression_Builder():\n",
        "  def getInfoPCOO(self):\n",
        "    #Pre-processing subgraph\n",
        "    featureSubgraph = []\n",
        "    tmp = []\n",
        "\n",
        "    for node_idx in torch.unique(data.edge_index[0]):\n",
        "      neighbors = data.edge_index[1][data.edge_index[0] == node_idx]\n",
        "      tmp.append(int(node_idx.item()))\n",
        "      for neighbor_idx in neighbors:\n",
        "        tmp.append(int(neighbor_idx.item()))\n",
        "\n",
        "    for node_idx in tmp:\n",
        "      featureSubgraph.append(data.x[node_idx].tolist())\n",
        "\n",
        "    #PCOO Processing\n",
        "    pcooData = []\n",
        "    numRows, numCols = len(featureSubgraph), len(featureSubgraph[0])\n",
        "\n",
        "    for row_idx in range(numRows):\n",
        "        non_zero_elements = [(col_idx, featureSubgraph[row_idx][col_idx]) for col_idx in range(numCols) if featureSubgraph[row_idx][col_idx] != 0]\n",
        "\n",
        "        if not non_zero_elements:\n",
        "            pcooData.append({'header': {'sor': 1, 'eor': 1, 'vld': 0}, 'body': {'col': 0, 'val': 0}})\n",
        "            continue\n",
        "\n",
        "        for i, (col_idx, value) in enumerate(non_zero_elements):\n",
        "            # Determine header bits for the element\n",
        "            sor = 1 if i == 0 else 0  # Start of row\n",
        "            eor = 1 if i == len(non_zero_elements) - 1 else 0  # End of row\n",
        "            vld = 1  # Set valid to 1 (assuming all values are used for computation)\n",
        "\n",
        "            if torch.is_tensor(value):\n",
        "                value = int(value.item())\n",
        "            else:\n",
        "                value = int(value)\n",
        "\n",
        "            # Combine header and body info for this element\n",
        "            header = {'sor': sor, 'eor': eor, 'vld': vld}\n",
        "            body = {\n",
        "                'col': col_idx,\n",
        "                'val': value\n",
        "            }\n",
        "            pcooData.append({'header': header, 'body': body})\n",
        "\n",
        "    return pcooData\n",
        "\n",
        "  def getConstraint(self):\n",
        "    pcooData = self.getInfoPCOO();\n",
        "    max_min_values = {field: {'max': float('-inf'), 'min': float('inf')} for field in ['sor', 'eor', 'vld', 'col', 'val']}\n",
        "\n",
        "    # Iterate through the array\n",
        "    for entry in pcooData:\n",
        "        header = entry['header']\n",
        "        body = entry['body']\n",
        "\n",
        "        # Update max and min values\n",
        "        for field in max_min_values:\n",
        "            value = header.get(field, body.get(field))\n",
        "            if value is not None:\n",
        "                max_min_values[field]['max'] = max(max_min_values[field]['max'], value)\n",
        "                max_min_values[field]['min'] = min(max_min_values[field]['min'], value)\n",
        "\n",
        "    bits_required_sor = bits_required(max_min_values['sor']['max'], max_min_values['sor']['min'], Configuration[\"PCOO\"][\"sorSigned\"]) if Configuration[\"PCOO\"][\"sorRequiredBit\"] == None else Configuration[\"PCOO\"][\"sorRequiredBit\"]\n",
        "    bits_required_eor = bits_required(max_min_values['eor']['max'], max_min_values['eor']['min'], Configuration[\"PCOO\"][\"eorSigned\"]) if Configuration[\"PCOO\"][\"eorRequiredBit\"] == None else Configuration[\"PCOO\"][\"eorRequiredBit\"]\n",
        "    bits_required_vld = bits_required(max_min_values['vld']['max'], max_min_values['vld']['min'], Configuration[\"PCOO\"][\"vldSigned\"]) if Configuration[\"PCOO\"][\"vldRequiredBit\"] == None else Configuration[\"PCOO\"][\"vldRequiredBit\"]\n",
        "    bits_required_col = bits_required(max_min_values['col']['max'], max_min_values['col']['min'], Configuration[\"PCOO\"][\"colSigned\"]) if Configuration[\"PCOO\"][\"colRequiredBit\"] == None else Configuration[\"PCOO\"][\"colRequiredBit\"]\n",
        "    bits_required_val = bits_required(max_min_values['val']['max'], max_min_values['val']['min'], Configuration[\"PCOO\"][\"valSigned\"]) if Configuration[\"PCOO\"][\"valRequiredBit\"] == None else Configuration[\"PCOO\"][\"valRequiredBit\"]\n",
        "\n",
        "    # Table Processing\n",
        "\n",
        "    raw =  {\n",
        "        'header': {\n",
        "            'sor':{\n",
        "                'max': max_min_values['sor']['max'],\n",
        "                'min': max_min_values['sor']['min'],\n",
        "                'bits': bits_required_sor,\n",
        "                'signed': Configuration[\"PCOO\"][\"sorSigned\"],\n",
        "                'shape': len(pcooData),\n",
        "                'count': len(pcooData)\n",
        "            },\n",
        "            'eor': {\n",
        "                'max': max_min_values['eor']['max'],\n",
        "                'min': max_min_values['eor']['min'],\n",
        "                'bits': bits_required_eor,\n",
        "                'signed': Configuration[\"PCOO\"][\"eorSigned\"],\n",
        "                'shape': len(pcooData),\n",
        "                'count': len(pcooData)\n",
        "            },\n",
        "            'vld': {\n",
        "                'max': max_min_values['vld']['max'],\n",
        "                'min': max_min_values['vld']['min'],\n",
        "                'bits': bits_required_vld,\n",
        "                'signed': Configuration[\"PCOO\"][\"vldSigned\"],\n",
        "                'shape': len(pcooData),\n",
        "                'count': len(pcooData)\n",
        "            }\n",
        "        },\n",
        "        'body': {\n",
        "            'col':{\n",
        "                'max': max_min_values['col']['max'],\n",
        "                'min': max_min_values['col']['min'],\n",
        "                'bits': bits_required_col,\n",
        "                'signed': Configuration[\"PCOO\"][\"colSigned\"],\n",
        "                'shape': len(pcooData),\n",
        "                'count': len(pcooData)\n",
        "            },\n",
        "            'val': {\n",
        "                'max': max_min_values['val']['max'],\n",
        "                'min': max_min_values['val']['min'],\n",
        "                'bits': bits_required_val,\n",
        "                'signed': Configuration[\"PCOO\"][\"valSigned\"],\n",
        "                'shape': len(pcooData),\n",
        "                'count': len(pcooData)\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "\n",
        "    table = PrettyTable()\n",
        "\n",
        "    # Define the table headers\n",
        "    table.field_names = [\"Category\", \"Subcategory\", \"Max\", \"Min\", \"Bits\", \"Signed\", \"Shape\", \"Count\"]\n",
        "\n",
        "    # Add rows to the table for each key in `raw`\n",
        "    for category, subcategories in raw.items():\n",
        "        if isinstance(subcategories, dict):\n",
        "            for subcategory, attributes in subcategories.items():\n",
        "                if isinstance(attributes, dict):  # Nested dictionary\n",
        "                    table.add_row([\n",
        "                        category,\n",
        "                        subcategory,\n",
        "                        attributes.get('max', '-'),\n",
        "                        attributes.get('min', '-'),\n",
        "                        attributes.get('bits', '-'),\n",
        "                        attributes.get('signed', '-'),\n",
        "                        attributes.get('shape', '-'),\n",
        "                        attributes.get('count', '-')\n",
        "                    ])\n",
        "                else:\n",
        "                    table.add_row([\n",
        "                        category,\n",
        "                        \"-\",\n",
        "                        subcategories.get('max', '-'),\n",
        "                        subcategories.get('min', '-'),\n",
        "                        subcategories.get('bits', '-'),\n",
        "                        subcategories.get('signed', '-'),\n",
        "                        subcategories.get('shape', '-'),\n",
        "                        subcategories.get('count', '-')\n",
        "                    ])\n",
        "                    break;\n",
        "        else:  # Leaf-level value\n",
        "            table.add_row([category, \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"])\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxwMK1uCBusb",
        "outputId": "addbf200-8a24-4ac3-cf06-94486a30202b"
      },
      "outputs": [],
      "source": [
        "PCOODataCompression = PCOO_Data_Compression_Builder()\n",
        "pcooRawData = PCOODataCompression.getInfoPCOO()\n",
        "print(PCOODataCompression.getConstraint())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3wiJXIpu6_x"
      },
      "source": [
        "# Manual Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StbNYyl4u9lW",
        "outputId": "f1794447-03b5-43a2-bd13-04d315b577bb"
      },
      "outputs": [],
      "source": [
        "#Start time\n",
        "start_time = time.time()\n",
        "\n",
        "a = GATModel.state_dict()\n",
        "# Using quantized parameter\n",
        "for k, v in a.items():\n",
        "  scaled_tensor, _ = scale_tensor(v,\n",
        "                                Configuration[\"BuildModel\"][\"scaleMin\"],\n",
        "                                Configuration[\"BuildModel\"][\"scaleMax\"],\n",
        "                                torch.int8)\n",
        "  a[k] = scaled_tensor\n",
        "\n",
        "# Wh\n",
        "# tmp_feature_torch = feature_matrix_1 * weight_1\n",
        "tmp_feature_torch= torch.matmul(data.x.to(dtype=torch.float32) , a['conv1.lin.weight'].to(dtype=torch.float32).t())\n",
        "print(f\"tmp_feature_matrix shape: {tmp_feature_torch.shape}\")\n",
        "\n",
        "# e_i_j = a_src.W.h_i || a_dst.W.h_j (TODO: DMVM + Calculate according to subgraph + COEF(e_i_j))\n",
        "tmp_info = []\n",
        "for i in range(data.edge_index.size(1)):\n",
        "  src_node = data.edge_index[0, i]\n",
        "  dst_node = data.edge_index[1, i]\n",
        "  feature_src_node = tmp_feature_torch[src_node] # W.h_i\n",
        "  feature_dst_node = tmp_feature_torch[dst_node] # W.h_j\n",
        "  att_src_node = torch.matmul(a['conv1.att_src'].to(dtype=torch.float32), feature_src_node.to(dtype=torch.float32)) # a_src.W.h_i\n",
        "  att_dst_node = torch.matmul(a['conv1.att_dst'].to(dtype=torch.float32), feature_dst_node.to(dtype=torch.float32)) # a_dst.W.h_j\n",
        "  tmp_info.append([src_node.item(), dst_node.item(), (att_src_node + att_dst_node).item()])\n",
        "\n",
        "tmp_info = sorted(tmp_info, key=lambda x: x[0])\n",
        "bit_required_e_i_j = bits_required(max([row[2] for row in tmp_info]), min([row[2] for row in tmp_info]), True)\n",
        "print(f\"tmp_info (a(Wh1||Wh2)): {len(tmp_info)}, max: {max([row[2] for row in tmp_info])}, min: {min([row[2] for row in tmp_info])}, bits_required: {bit_required_e_i_j}\")\n",
        "\n",
        "# Group by src_node\n",
        "group_by_src_node = defaultdict(list)\n",
        "for row in tmp_info:\n",
        "  group_by_src_node[row[0]].append(row)\n",
        "\n",
        "# Softmax + LeakyRelu\n",
        "max_numerators = 0\n",
        "max_denominators = 0\n",
        "for i, rows in group_by_src_node.items():\n",
        "  numerators = [2**((max(row[2], 0) /(2**(bit_required_e_i_j - 8)))) for row in rows] #TODO\n",
        "  # print(\"Divide by\", bit_required_e_i_j - 8)\n",
        "  denominator = sum(numerators) #TODO\n",
        "  max_numerators = max(max_numerators, max(numerators))\n",
        "  max_denominators = max(max_denominators, denominator)\n",
        "  for idx, row in enumerate(rows):\n",
        "    alpha_i_j = numerators[idx] / denominator #TODO\n",
        "    row.append(alpha_i_j) #[src_node, dst_node, a(Wh1||Wh2), alpha_i_j] -> group_src_node\n",
        "\n",
        "print(f\"max_numerators of softmax: {max_numerators}, bits_required: {bits_required(max_numerators, 0)}\")\n",
        "print(f\"max_denominators of softmax: {max_denominators}, bits_required: {bits_required(max_denominators, 0)}\")\n",
        "\n",
        "\n",
        "group_to_matrix = [row for group in group_by_src_node.values() for row in group] #[[0, 633, -186704.0, 0.3333333333333333], [0, 1862, -182407.0, 0.3333333333333333], [0, 2582, -144662.0, 0.3333333333333333], [1, 2, -100041.0, 0.040421365723215065],...]\n",
        "result_feature_torch = torch.zeros(tmp_feature_torch.shape[0], tmp_feature_torch.shape[1]).to(dtype=torch.float32) #h'\n",
        "\n",
        "\n",
        "for row in group_to_matrix:\n",
        "   result_feature_torch[row[0]] = result_feature_torch[row[0]] + torch.tensor(row[3]).to(dtype=torch.float32) * tmp_feature_torch[row[1]].to(dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "print(f\"h' after first layer: {result_feature_torch.shape}, max: {torch.max(result_feature_torch)}, min: {torch.min(result_feature_torch)}\")\n",
        "print(result_feature_torch)\n",
        "print(f\"result_feature shape: {result_feature_torch.shape}, in range: {((result_feature_torch >= -127) & (result_feature_torch <= 127)).sum().item()}\")\n",
        "\n",
        "# Quantized\n",
        "result_feature_torch, _ = scale_tensor(result_feature_torch,\n",
        "                                                       Configuration[\"BuildModel\"][\"scaleMin\"],\n",
        "                                                       Configuration[\"BuildModel\"][\"scaleMax\"],\n",
        "                                                       torch.int8)\n",
        "print(\"After quantized: \", result_feature_torch)\n",
        "print(f\"result_feature shape: {result_feature_torch.shape}, in range: {((result_feature_torch >= -127) & (result_feature_torch <= 127)).sum().item()}\")\n",
        "\n",
        "\n",
        "# Wh\n",
        "# tmp_feature_torch = feature_matrix_2 * weight_2\n",
        "tmp_feature_torch= torch.matmul(result_feature_torch.to(dtype=torch.float32) , a['conv2.lin.weight'].to(dtype=torch.float32).t())\n",
        "print(f\"tmp_feature_matrix shape: {tmp_feature_torch.shape}\")\n",
        "\n",
        "\n",
        "# e_i_j = a_src.W.h_i || a_dst.W.h_j\n",
        "tmp_info = []\n",
        "for i in range(data.edge_index.size(1)):\n",
        "  src_node = data.edge_index[0, i]\n",
        "  dst_node = data.edge_index[1, i]\n",
        "  feature_src_node = tmp_feature_torch[src_node] # W.h_i\n",
        "  feature_dst_node = tmp_feature_torch[dst_node] # W.h_j\n",
        "  att_src_node = torch.matmul(a['conv2.att_src'].to(dtype=torch.float32), feature_src_node.to(dtype=torch.float32)) # a_src.W.h_i\n",
        "  att_dst_node = torch.matmul(a['conv2.att_dst'].to(dtype=torch.float32), feature_dst_node.to(dtype=torch.float32)) # a_dst.W.h_j\n",
        "  tmp_info.append([src_node.item(), dst_node.item(), (att_src_node + att_dst_node).item()])\n",
        "\n",
        "tmp_info = sorted(tmp_info, key=lambda x: x[0])\n",
        "bit_required_e_i_j = bits_required(max([row[2] for row in tmp_info]), min([row[2] for row in tmp_info]), True)\n",
        "print(f\"tmp_info (a(Wh1||Wh2)): {len(tmp_info)}, max: {max([row[2] for row in tmp_info])}, min: {min([row[2] for row in tmp_info])}, bits_required: {bit_required_e_i_j}\")\n",
        "\n",
        "# Group by src_node\n",
        "group_by_src_node = defaultdict(list)\n",
        "for row in tmp_info:\n",
        "  group_by_src_node[row[0]].append(row)\n",
        "\n",
        "# Softmax + LeakyRelu\n",
        "max_numerators = 0\n",
        "max_denominators = 0\n",
        "for i, rows in group_by_src_node.items():\n",
        "  numerators = [2**((max(row[2], 0) /(2**(bit_required_e_i_j - 8)))) for row in rows]\n",
        "  denominator = sum(numerators)\n",
        "  max_numerators = max(max_numerators, max(numerators))\n",
        "  max_denominators = max(max_denominators, denominator)\n",
        "  for idx, row in enumerate(rows):\n",
        "    alpha_i_j = numerators[idx] / denominator\n",
        "    row.append(alpha_i_j) #[src_node, dst_node, a(Wh1||Wh2), alpha_i_j] -> group_src_node\n",
        "\n",
        "print(f\"max_numerators of softmax: {max_numerators}, bits_required: {bits_required(max_numerators, 0)}\")\n",
        "print(f\"max_denominators of softmax: {max_denominators}, bits_required: {bits_required(max_denominators, 0)}\")\n",
        "\n",
        "\n",
        "group_to_matrix = [row for group in group_by_src_node.values() for row in group] #[[0, 633, -186704.0, 0.3333333333333333], [0, 1862, -182407.0, 0.3333333333333333], [0, 2582, -144662.0, 0.3333333333333333], [1, 2, -100041.0, 0.040421365723215065],...]\n",
        "result_feature_torch = torch.zeros(tmp_feature_torch.shape[0], tmp_feature_torch.shape[1]).to(dtype=torch.float32) #h'\n",
        "\n",
        "\n",
        "for row in group_to_matrix:\n",
        "   result_feature_torch[row[0]] = result_feature_torch[row[0]] + torch.tensor(row[3]).to(dtype=torch.float32) * tmp_feature_torch[row[1]].to(dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "print(f\"h' after second layer: {result_feature_torch.shape}, max: {torch.max(result_feature_torch)}, min: {torch.min(result_feature_torch)}\")\n",
        "print(result_feature_torch)\n",
        "\n",
        "# Output comparison\n",
        "my_output = torch.argmax(result_feature_torch, dim=1)\n",
        "print(\"Classification: \", my_output)\n",
        "print(\"Correct result\", data.y)\n",
        "\n",
        "correct_count = (my_output == data.y).sum().item()\n",
        "print(f\"Correct count: {correct_count} / {tmp_feature_torch.shape[0]}, acc: {correct_count / tmp_feature_torch.shape[0]}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time to run test: {end_time - start_time:.6f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixe4TFGHY9Qz"
      },
      "source": [
        "#Export File Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "smuuz9yOV98w"
      },
      "outputs": [],
      "source": [
        "def format_number(value):\n",
        "    # Convert the number to a string\n",
        "    value_str = str(value)\n",
        "    # Check if the string ends with '.0' and remove it\n",
        "    if value_str.endswith('.0'):\n",
        "        value_str = value_str[:-2]\n",
        "    return value_str\n",
        "\n",
        "\n",
        "def initFileConfig(isLayer1 = False):\n",
        "  option = 2\n",
        "  if isLayer1:\n",
        "    option = 1\n",
        "  FirstLayerStructure = {\n",
        "      \"input_dir\" : {\n",
        "        \"name\": f\"exports/layer_{option}/input/\",\n",
        "        \"child\": {\n",
        "          \"weight\": \"weight.txt\",\n",
        "          \"attention\": \"a.txt\",\n",
        "          \"col_idx_and_value\": \"h_data.txt\",\n",
        "          \"node_info\": \"node_info.txt\",\n",
        "          \"input_value\": \"h_data.txt\",\n",
        "          \"graph_index\": \"graph_index.txt\",\n",
        "          \"merge_weight\": \"merge_weight.txt\",\n",
        "        }\n",
        "      },\n",
        "      \"output_dir\": {\n",
        "        \"name\": f\"exports/layer_{option}/output/\",\n",
        "        \"child\": {\n",
        "          \"SPMM_dir\": {\n",
        "            \"name\": \"SPMM/\",\n",
        "            \"child\": {\n",
        "                \"wh\": \"WH.txt\"\n",
        "            }\n",
        "          },\n",
        "          \"DMVM_dir\": {\n",
        "            \"name\": \"DMVM/\",\n",
        "            \"child\": {\n",
        "                \"coef\": \"COEF.txt\",\n",
        "                \"DMVM\": \"DMVM.txt\"\n",
        "            }\n",
        "          },\n",
        "          \"softmax_dir\": {\n",
        "            \"name\": \"softmax/\",\n",
        "            \"child\": {\n",
        "                \"num_nodes\": \"num_nodes.txt\",\n",
        "                \"alpha\": \"ALPHA.txt\",\n",
        "                \"dividend\": \"DIVIDEND.txt\",\n",
        "                \"divisor\": \"DIVISOR.txt\"\n",
        "            }\n",
        "          },\n",
        "          \"aggregator_dir\": {\n",
        "            \"name\": \"aggregator/\",\n",
        "            \"child\": {\n",
        "                \"new_feature\": \"new_feature.txt\"\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "  FLFileConfig = {\n",
        "      1:{\n",
        "        \"input_dir\": FirstLayerStructure['input_dir']['name'],\n",
        "        \"attention_file\": FirstLayerStructure['input_dir']['child']['attention'],\n",
        "        \"weight_file\": FirstLayerStructure['input_dir']['child']['weight'],\n",
        "        \"col_idx_and_value_file\": FirstLayerStructure['input_dir']['child']['col_idx_and_value'],\n",
        "        \"node_info_file\": FirstLayerStructure['input_dir']['child']['node_info'],\n",
        "        \"graph_index_file\": FirstLayerStructure['input_dir']['child']['graph_index'],\n",
        "        \"merge_weight_file\": FirstLayerStructure['input_dir']['child']['merge_weight'],\n",
        "        \"output_spmm_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['SPMM_dir']['name']}\",\n",
        "        \"output_wh_file\": FirstLayerStructure['output_dir']['child']['SPMM_dir']['child']['wh'],\n",
        "        \"output_dmvm_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['DMVM_dir']['name']}\",\n",
        "        \"output_coef_file\": FirstLayerStructure['output_dir']['child']['DMVM_dir']['child']['coef'],\n",
        "        \"output_dmvm_file\": FirstLayerStructure['output_dir']['child']['DMVM_dir']['child']['DMVM'],\n",
        "        \"output_softmax_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['softmax_dir']['name']}\",\n",
        "        \"output_num_nodes_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['num_nodes'],\n",
        "        \"output_alpha_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['alpha'],\n",
        "        \"output_dividend_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['dividend'],\n",
        "        \"output_divisor_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['divisor'],\n",
        "        \"output_aggregator_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['aggregator_dir']['name']}\",\n",
        "        \"output_new_feature_file\": FirstLayerStructure['output_dir']['child']['aggregator_dir']['child']['new_feature'],\n",
        "      },\n",
        "      2: {\n",
        "        \"input_dir\": FirstLayerStructure['input_dir']['name'],\n",
        "        \"attention_file\": FirstLayerStructure['input_dir']['child']['attention'],\n",
        "        \"weight_file\": FirstLayerStructure['input_dir']['child']['weight'],\n",
        "        \"input_value_file\": FirstLayerStructure['input_dir']['child']['input_value'],\n",
        "        \"merge_weight_file\": FirstLayerStructure['input_dir']['child']['merge_weight'],\n",
        "        \"output_spmm_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['SPMM_dir']['name']}\",\n",
        "        \"output_wh_file\": FirstLayerStructure['output_dir']['child']['SPMM_dir']['child']['wh'],\n",
        "        \"output_dmvm_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['DMVM_dir']['name']}\",\n",
        "        \"output_coef_file\": FirstLayerStructure['output_dir']['child']['DMVM_dir']['child']['coef'],\n",
        "        \"output_dmvm_file\": FirstLayerStructure['output_dir']['child']['DMVM_dir']['child']['DMVM'],\n",
        "        \"output_softmax_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['softmax_dir']['name']}\",\n",
        "        \"output_num_nodes_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['num_nodes'],\n",
        "        \"output_alpha_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['alpha'],\n",
        "        \"output_dividend_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['dividend'],\n",
        "        \"output_divisor_file\": FirstLayerStructure['output_dir']['child']['softmax_dir']['child']['divisor'],\n",
        "        \"output_aggregator_dir\": f\"{FirstLayerStructure['output_dir']['name']}{FirstLayerStructure['output_dir']['child']['aggregator_dir']['name']}\",\n",
        "        \"output_new_feature_file\": FirstLayerStructure['output_dir']['child']['aggregator_dir']['child']['new_feature'],\n",
        "      }\n",
        "  }\n",
        "  return FLFileConfig[option]\n",
        "\n",
        "\n",
        "def exportToFilesFirstLayer(input_subgraph, layer_idx, notLayer1InputTensor= None):\n",
        "  # FilesConfiguration\n",
        "  FLFileConfig = None\n",
        "  if notLayer1InputTensor != None:\n",
        "    FLFileConfig = initFileConfig(False)\n",
        "  else:\n",
        "    FLFileConfig = initFileConfig(True)\n",
        "\n",
        "\n",
        "  # Input\n",
        "  a = GATModel.state_dict()\n",
        "  for k, v in a.items():\n",
        "    scaled_tensor, scale_back_fn = scale_tensor(v,\n",
        "                                                Configuration[\"LParamQuan\"][\"scaleMin\"],\n",
        "                                                Configuration[\"LParamQuan\"][\"scaleMax\"],\n",
        "                                                torch.int8)\n",
        "    if scaled_tensor.ndim == 3:\n",
        "      scaled_tensor = scaled_tensor.reshape(scaled_tensor.shape[0], -1)\n",
        "    a[k] = scaled_tensor\n",
        "\n",
        "  input_dir = FLFileConfig['input_dir']\n",
        "  os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "  # Attention File\n",
        "  att_src = [element[0] for element in a[f\"conv{layer_idx+1}.att_src\"].t().tolist()]\n",
        "  att_dst = [element[0] for element in a[f\"conv{layer_idx+1}.att_dst\"].t().tolist()]\n",
        "  att_total = att_src + att_dst\n",
        "  att_total_binary = intToBinaryArray(att_total, 8)\n",
        "  print(\"Att total binary\", att_total_binary)\n",
        "  write_array_to_file(att_total_binary, createPath(input_dir, FLFileConfig['attention_file']))\n",
        "\n",
        "  # Weight\n",
        "  weight = a[f\"conv{layer_idx+1}.lin.weight\"].t().tolist()\n",
        "  weight_binary = intToBinaryMatrix(weight, 8)\n",
        "  print(\"Weight binary\", weight_binary)\n",
        "  write_matrix_to_file(weight_binary, createPath(input_dir, FLFileConfig['weight_file']))\n",
        "  append_array_to_file(att_total_binary, createPath(input_dir, FLFileConfig['weight_file']))\n",
        "\n",
        "  # # Weight Merge\n",
        "  # if notLayer1InputTensor == None:\n",
        "  #   att_src_value_1 = [element[0] for element in a[f\"conv1.att_src\"].t().tolist()]\n",
        "  #   att_dst_value_1 = [element[0] for element in a[f\"conv1.att_dst\"].t().tolist()]\n",
        "  #   att_total_value_1 = att_src_value_1 + att_dst_value_1\n",
        "  #   att_total_binary_value_1 = intToBinaryArray(att_total_value_1, 8)\n",
        "  #   print(\"Att binary 1\", att_total_binary_value_1)\n",
        "\n",
        "  #   att_src_value_2 = [element[0] for element in a[f\"conv2.att_src\"].t().tolist()]\n",
        "  #   att_dst_value_2 = [element[0] for element in a[f\"conv2.att_dst\"].t().tolist()]\n",
        "  #   att_total_value_2 = att_src_value_2 + att_dst_value_2\n",
        "  #   att_total_binary_value_2 = intToBinaryArray(att_total_value_2, 8)\n",
        "  #   print(\"Att binary 2\", att_total_binary_value_2)\n",
        "\n",
        "  #   weight_value_1 = a[f\"conv1.lin.weight\"].t().tolist()\n",
        "  #   weight_binary_value_1 = intToBinaryMatrix(weight_value_1, 8)\n",
        "  #   weight_binary_value_1_list = [item for row in weight_binary_value_1 for item in row]\n",
        "\n",
        "  #   weight_value_2 = a[f\"conv2.lin.weight\"].t().tolist()\n",
        "  #   weight_binary_value_2 = intToBinaryMatrix(weight_value_2, 8)\n",
        "  #   weight_binary_value_2_list = [item for row in weight_binary_value_2 for item in row]\n",
        "  #   print(\"Weight binary 2\", weight_binary_value_2)\n",
        "  #   print()\n",
        "\n",
        "  #   write_array_to_file(weight_binary_value_1_list + att_total_binary_value_1 + weight_binary_value_2_list + att_total_binary_value_2, createPath(input_dir, FLFileConfig['merge_weight_file']))\n",
        "\n",
        "  # Layer 1 only\n",
        "  if notLayer1InputTensor == None:\n",
        "\n",
        "    # Col Index & Value\n",
        "    col_index_binary = intToBinaryArray(col_index_raw, bits_required_col_index)\n",
        "    value_binary = intToBinaryArray(value_raw, bits_required_value)\n",
        "    col_index_value_binary = [a + b for a, b in zip(col_index_binary, value_binary)]\n",
        "    write_array_to_file(col_index_value_binary, createPath(input_dir, FLFileConfig['col_idx_and_value_file']))\n",
        "\n",
        "    # Node Info\n",
        "    # TODO: FIX THIS change bits_required_col_index to bits_required_node_info_row_length\n",
        "    node_info_binray = [row[-1] for row in resolveNodeInfo(node_info_raw, bits_required_col_index, bits_required_node_info_num_of_nodes, bits_required_node_info_flag)]\n",
        "    write_array_to_file(node_info_binray, createPath(input_dir, FLFileConfig['node_info_file']))\n",
        "\n",
        "\n",
        "    # Subgraph Index\n",
        "    print(f\"Check by huynguyenn subgraph_index: {len(node_info_binray)}, bits_required: {bits_required(0, len(node_info_binray)-1, False)}\")\n",
        "    # subgraph_index_info = []\n",
        "    # maximum_subgraph_index = len(node_info_binray)\n",
        "    # bits_required_subgraph_index = bits_required(0, len(node_info_binray)-1, False)\n",
        "    # for src_idx in torch.unique(data.edge_index[0]):\n",
        "    #   neighbors_idx_arr = data.edge_index[1][data.edge_index[0] == src_idx]\n",
        "    #   subgraph_index_info.append(src_idx.item())\n",
        "    #   for neighbor_idx in neighbors_idx_arr:\n",
        "    #     subgraph_index_info.append(neighbor_idx.item())\n",
        "    # incremental_count_index_info = list(range(maximum_subgraph_index))\n",
        "    # print(\"Check length subgraph index:\", len(subgraph_index_info),len(incremental_count_index_info))\n",
        "    # # print(\"Subgraph_index_info\", subgraph_index_info)\n",
        "    # # print(\"Incremental_count_index_info\", incremental_count_index_info)\n",
        "    # subgraph_matrix_info = [subgraph_index_info, incremental_count_index_info]\n",
        "    # print(\"Subgraph Matrix Info\", subgraph_matrix_info)\n",
        "    # subgraph_index_binary = []\n",
        "    # for src_idx in torch.unique(data.edge_index[0]):\n",
        "    #   index_arr = [subgraph_matrix_info[1][i] for i in range(len(subgraph_matrix_info[0])) if subgraph_matrix_info[0][i] == src_idx]\n",
        "    #   # print(\"Index arr\", len(index_arr), index_arr)\n",
        "    #   for i, value in enumerate(index_arr):\n",
        "    #     if i == 0:\n",
        "    #       subgraph_index_binary.append(int_to_n_bit_binary_with_flags(value, bits_required_subgraph_index, True, False))\n",
        "    #     elif i == len(index_arr) - 1:\n",
        "    #       subgraph_index_binary.append(int_to_n_bit_binary_with_flags(value, bits_required_subgraph_index, False, True))\n",
        "    #     else:\n",
        "    #       subgraph_index_binary.append(int_to_n_bit_binary_with_flags(value, bits_required_subgraph_index, False, False))\n",
        "\n",
        "    # print(\"Subgraph binary\", len(subgraph_index_binary))\n",
        "    # write_array_to_file(subgraph_index_binary, createPath(input_dir, FLFileConfig['graph_index_file']))\n",
        "\n",
        "\n",
        "    print(\"Bits required col_index: \", bits_required_col_index)\n",
        "    print(\"Bits required value: \", bits_required_value)\n",
        "    # TODO: FIX THIS\n",
        "    print(\"Bits required row_length\", bits_required_col_index)\n",
        "    print(\"Bits required num_of_nodes\", bits_required_node_info_num_of_nodes)\n",
        "    print(\"Bits required node_info_flag\", bits_required_node_info_flag)\n",
        "    print(\"Bits required h_data: \", bits_required_col_index + bits_required_value)\n",
        "    print(\"Bits required node_info: \", bits_required_col_index + bits_required_node_info_num_of_nodes + bits_required_node_info_flag)\n",
        "    # print(\"Bits required subgraph_index: \", bits_required_subgraph_index + 2)\n",
        "  else: #Layer n (with n >= 2)\n",
        "    h_matrix = []\n",
        "    for _, value in input_subgraph.items():\n",
        "        h_matrix.append(value['src_h'].tolist())\n",
        "        for _, neighbor_value in value['neighbors'].items():\n",
        "            h_matrix.append(neighbor_value['nb_h'].tolist())\n",
        "    h_matrix_binary = intToBinaryMatrix([[format_number(value) for value in row] for row in h_matrix], 8)\n",
        "    write_matrix_to_file(h_matrix_binary, createPath(input_dir, FLFileConfig['input_value_file']))\n",
        "\n",
        "  # Output\n",
        "  #SPMM\n",
        "  output_dir_SPMM = FLFileConfig['output_spmm_dir']\n",
        "  os.makedirs(output_dir_SPMM, exist_ok=True)\n",
        "  wh = []\n",
        "  for _, value in input_subgraph.items():\n",
        "      wh.append(value['src_z'])\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          wh.append(neighbor_value['nb_z'])\n",
        "\n",
        "  write_array_to_file([format_number(x) for x in torch.cat(wh, dim=0).tolist()], createPath(output_dir_SPMM, FLFileConfig['output_wh_file']))\n",
        "  print(\"=> Max SPMM\", max(torch.cat(wh, dim=0).tolist()))\n",
        "  print(\"=> Min SPMM\", min(torch.cat(wh, dim=0).tolist()))\n",
        "\n",
        "  #DMVM\n",
        "  output_dir_DMVM = FLFileConfig['output_dmvm_dir']\n",
        "  os.makedirs(output_dir_DMVM, exist_ok=True)\n",
        "  coef = []\n",
        "  dmvm = []\n",
        "  for _, value in input_subgraph.items():\n",
        "      dmvm.append(value['src_e_src'].squeeze(0))\n",
        "      dmvm.append(value['src_e_dst'].squeeze(0))\n",
        "      coef.append(value['src_e_i_j'].squeeze(0))\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          dmvm.append(neighbor_value['nb_e'].squeeze(0))\n",
        "          coef.append(neighbor_value['nb_e_i_j'].squeeze(0))\n",
        "\n",
        "  write_array_to_file([format_number(x) for x in torch.cat(coef, dim=0).tolist()], createPath(output_dir_DMVM, FLFileConfig['output_coef_file']))\n",
        "  write_array_to_file([format_number(x) for x in torch.cat(dmvm, dim=0).tolist()], createPath(output_dir_DMVM, FLFileConfig['output_dmvm_file']))\n",
        "  print(\"=> Max DMVM\", max(torch.cat(dmvm, dim=0).tolist()))\n",
        "  print(\"=> Min DMVM\", min(torch.cat(dmvm, dim=0).tolist()))\n",
        "  print(\"=> Max COEF\", max(torch.cat(coef, dim=0).tolist()))\n",
        "  print(\"=> Min COEF\", min(torch.cat(coef, dim=0).tolist()))\n",
        "\n",
        "  #SOFTMAX\n",
        "  output_dir_softmax = FLFileConfig['output_softmax_dir']\n",
        "  os.makedirs(output_dir_softmax, exist_ok=True)\n",
        "  alpha = []\n",
        "  dividend = []\n",
        "  divisor = []\n",
        "  nodes = []\n",
        "  for _, value in input_subgraph.items():\n",
        "      nodes.append(value['nodes'])\n",
        "      alpha.append(value['src_alpha'].squeeze(0))\n",
        "      dividend.append(value['src_dividend'].squeeze(0))\n",
        "      divisor.append(value['divisor'].squeeze(0))\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          alpha.append(neighbor_value['nb_alpha'].squeeze(0))\n",
        "          dividend.append(neighbor_value['nb_dividend'].squeeze(0))\n",
        "\n",
        "  write_array_to_file(nodes, createPath(output_dir_softmax, FLFileConfig['output_num_nodes_file']))\n",
        "  write_array_to_file(torch.cat(alpha, dim=0).tolist(), createPath(output_dir_softmax, FLFileConfig['output_alpha_file']))\n",
        "  write_array_to_file([format_number(x) for x in torch.cat(dividend, dim=0).tolist()], createPath(output_dir_softmax, FLFileConfig['output_dividend_file']))\n",
        "  write_array_to_file([format_number(x) for x in torch.cat(divisor, dim=0).tolist()], createPath(output_dir_softmax, FLFileConfig['output_divisor_file']))\n",
        "  print(\"=> Max alpha\", max(torch.cat(alpha, dim=0).tolist()))\n",
        "  print(\"=> Min alpha\", min(torch.cat(alpha, dim=0).tolist()))\n",
        "  print(\"=> Max dividend\", max(torch.cat(dividend, dim=0).tolist()))\n",
        "  print(\"=> Min dividend\", min(torch.cat(dividend, dim=0).tolist()))\n",
        "  print(\"=> Max divisor\", max(torch.cat(divisor, dim=0).tolist()))\n",
        "  print(\"=> Min divisor\", min(torch.cat(divisor, dim=0).tolist()))\n",
        "\n",
        "  #aggregator\n",
        "  output_dir_aggregator = FLFileConfig['output_aggregator_dir']\n",
        "  os.makedirs(output_dir_aggregator, exist_ok=True)\n",
        "  new_feature = []\n",
        "  for _, value in input_subgraph.items():\n",
        "      new_feature.append(value['new_h'].squeeze(0))\n",
        "  write_array_to_file(torch.cat(new_feature, dim=0).tolist(), createPath(output_dir_aggregator, FLFileConfig['output_new_feature_file']))\n",
        "  print(\"=> Max new_feature\", max(torch.cat(new_feature, dim=0).tolist()))\n",
        "  print(\"=> Min new_feature\", min(torch.cat(new_feature, dim=0).tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qzdrsTzpLacx"
      },
      "outputs": [],
      "source": [
        "test_new_feature = None\n",
        "test_output = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9HWSrN5lQ-B",
        "outputId": "1359e5f4-a708-40f9-d99e-6760b87b55c6"
      },
      "outputs": [],
      "source": [
        "print(511.99999/4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiXdtRAiIcKx"
      },
      "source": [
        "#**Simulation & Export to Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB3NUCBOi7Vb",
        "outputId": "ccf675ab-d435-4735-8fda-ac1c5655f618"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import json\n",
        "\n",
        "NUM_OF_LAYERS = 2\n",
        "\n",
        "input_layer = data.x.to(dtype=torch.float32)\n",
        "a = GATModel.state_dict()\n",
        "\n",
        "IS_HARDWARE_HANDLE_OUTPUT_LAYER_1 = True\n",
        "\n",
        "# Getting quantized parameters\n",
        "for k, v in a.items():\n",
        "  scaled_tensor, _ = scale_tensor(v,\n",
        "                                Configuration[\"BuildModel\"][\"scaleMin\"],\n",
        "                                Configuration[\"BuildModel\"][\"scaleMax\"],\n",
        "                                torch.int8)\n",
        "  a[k] = scaled_tensor\n",
        "\n",
        "for layer_idx in range(NUM_OF_LAYERS):\n",
        "  print(f\"----------------------------------------Start Layer {layer_idx + 1}-------------------------------------------\")\n",
        "  #SPMM: Calculate WH(= z) according to subgraph\n",
        "  print(a[f\"conv{layer_idx+1}.lin.weight\"].to(dtype=torch.float32).t())\n",
        "  print(a[f\"conv{layer_idx+1}.lin.weight\"].to(dtype=torch.float32).t().shape)\n",
        "  z_layer1= torch.matmul(input_layer , a[f\"conv{layer_idx+1}.lin.weight\"].to(dtype=torch.float32).t()) #2708 * 16\n",
        "  e_idx = data.edge_index\n",
        "  subgraph = {}\n",
        "\n",
        "  #DMVM: Calculate DMVM + e\n",
        "  for node_idx in torch.unique(e_idx[0]):\n",
        "      neighbors_idx_arr = e_idx[1][e_idx[0] == node_idx]\n",
        "      src_dmvm_src = torch.matmul(a[f\"conv{layer_idx+1}.att_src\"].to(dtype=torch.float32), z_layer1[node_idx].to(dtype=torch.float32)) # z1 x a1 = e1\n",
        "      src_dmvm_dst = torch.matmul(a[f\"conv{layer_idx+1}.att_dst\"].to(dtype=torch.float32), z_layer1[node_idx].to(dtype=torch.float32)) # z1 x a2 = e1\n",
        "\n",
        "      subgraph[int(node_idx)] = {\n",
        "          'src_h': input_layer[node_idx],\n",
        "          'src_z': z_layer1[node_idx],\n",
        "          'src_e_src': src_dmvm_src,\n",
        "          'src_e_dst': src_dmvm_dst,\n",
        "          'src_e_i_j': torch.relu(src_dmvm_src + src_dmvm_dst), # e1 + e1' = e11\n",
        "          # 'src_e_i_j': torch.nn.functional.leaky_relu(src_dmvm_src + src_dmvm_dst, negative_slope=0.01), # e1 + e1' = e11\n",
        "          'neighbors': {},\n",
        "      }\n",
        "\n",
        "      for neighbor_idx in neighbors_idx_arr:\n",
        "          neighbor_dmvm = torch.matmul(a[f\"conv{layer_idx+1}.att_dst\"].to(dtype=torch.float32), z_layer1[neighbor_idx].to(dtype=torch.float32)) #z2 x a2 = e2, z3 x a2 = e3,....\n",
        "          subgraph[int(node_idx)]['neighbors'][int(neighbor_idx)] = {\n",
        "              'nb_h': input_layer[neighbor_idx],\n",
        "              'nb_z': z_layer1[neighbor_idx],\n",
        "              'nb_e': neighbor_dmvm,\n",
        "              'nb_e_i_j': torch.relu(src_dmvm_src + neighbor_dmvm) #e1 + e2 = e12, e1 + e3 = e13,....\n",
        "              # 'nb_e_i_j': torch.nn.functional.leaky_relu(src_dmvm_src + neighbor_dmvm, negative_slope=0.01), #e1 + e2 = e12, e1 + e3 = e13,....\n",
        "          }\n",
        "\n",
        "  # Check max, min and calculate bits to reduce\n",
        "  values = []\n",
        "  for key, value in subgraph.items():\n",
        "      values.append(value['src_e_i_j'])\n",
        "      for neighbor_key, neighbor_value in value['neighbors'].items():\n",
        "          values.append(neighbor_value['nb_e_i_j'])\n",
        "  all_values = torch.cat(values)\n",
        "  min_value = torch.min(all_values)\n",
        "  max_value = torch.max(all_values)\n",
        "  #Important dont delete\n",
        "  bits_e_i_j = bits_required(max_value, min_value, True)\n",
        "  # print(min_value, max_value, bits_e_i_j, bits_e_i_j-8, max_value / 2**(bits_e_i_j-8)) #Citeseer: /2^12\n",
        "  print(\"Max COE:\", max_value)\n",
        "  print(\"Min COE:\", min_value)\n",
        "  print(\"Remove # of bits\", bits_e_i_j-8)\n",
        "\n",
        "  for _, value in subgraph.items():\n",
        "      value['src_e_i_j'] = value['src_e_i_j'] // (2**(bits_e_i_j - 8))\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          neighbor_value['nb_e_i_j'] = neighbor_value['nb_e_i_j'] // (2**(bits_e_i_j - 8))\n",
        "\n",
        "  #Softmax: Calculate DIVIDEND + DIVISOR + ALPHA + num_nodes\n",
        "  for key, value in subgraph.items():\n",
        "      divisor = 0\n",
        "      num_of_node = 1\n",
        "      src_dividend = 2**value['src_e_i_j']\n",
        "      divisor += src_dividend.double()\n",
        "      value['src_dividend'] = src_dividend\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          nb_dividend = 2**neighbor_value['nb_e_i_j']\n",
        "          neighbor_value['nb_dividend'] = nb_dividend\n",
        "          num_of_node += 1\n",
        "          divisor += nb_dividend.double()\n",
        "      value['divisor'] = divisor\n",
        "      value['nodes'] = num_of_node\n",
        "\n",
        "  for _, value in subgraph.items():\n",
        "      src_alpha = value['src_dividend'] / value['divisor']\n",
        "      value['src_alpha'] = src_alpha\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          nb_alpha = neighbor_value['nb_dividend'] / value['divisor']\n",
        "          neighbor_value['nb_alpha'] = nb_alpha\n",
        "\n",
        "  for _, value in subgraph.items():\n",
        "      total = 0\n",
        "      total = value['src_alpha'] * value['src_z']\n",
        "      for _, neighbor_value in value['neighbors'].items():\n",
        "          total += neighbor_value['nb_alpha'] * neighbor_value['nb_z']\n",
        "      value['new_h'] = torch.relu(total)\n",
        "      # value['new_h'] = torch.nn.functional.leaky_relu(total, negative_slope=0.01)\n",
        "\n",
        "  # Export to file here\n",
        "  # Handle input of next layer\n",
        "  feature_next_layer = []\n",
        "  # Use for isolated node\n",
        "  print(f\"Subgraph Len: {len(subgraph)} / {data.x.shape[0]}\")\n",
        "  for idx in range(data.x.shape[0]):\n",
        "    if idx in subgraph:\n",
        "      feature_next_layer.append(subgraph[idx]['new_h'].squeeze(0))\n",
        "    else:\n",
        "      feature_next_layer.append(z_layer1[idx].to(dtype=torch.float64))\n",
        "  feature_next_layer_tensor = torch.stack(feature_next_layer)\n",
        "  print(feature_next_layer_tensor, feature_next_layer_tensor.shape)\n",
        "  # To remove just test only\n",
        "  if layer_idx == 0:\n",
        "    test_new_feature = feature_next_layer_tensor\n",
        "  else:\n",
        "    test_output = feature_next_layer_tensor\n",
        "  # Export data\n",
        "  if layer_idx == 0:\n",
        "    exportToFilesFirstLayer(subgraph, layer_idx, None)\n",
        "  else:\n",
        "    exportToFilesFirstLayer(subgraph, layer_idx, input_layer)\n",
        "\n",
        "  # Break when calculation is done\n",
        "  if layer_idx + 1 == NUM_OF_LAYERS:\n",
        "    classification_output = torch.argmax(feature_next_layer_tensor, dim=1)\n",
        "    classification_correct = classification_output[data.test_mask] == data.y[data.test_mask]\n",
        "    test_acc = int(classification_correct.sum()) / int(data.test_mask.sum())\n",
        "    print(\"Classification: \", classification_output)\n",
        "    print(\"Correct result\", data.y)\n",
        "\n",
        "    correct_count = (classification_output == data.y).sum().item()\n",
        "    print(f\"Correct count final: {int(classification_correct.sum())} / {int(data.test_mask.sum())}, acc: {int(classification_correct.sum()) / int(data.test_mask.sum())}\")\n",
        "    break;\n",
        "  # Quantize and assign to input\n",
        "  start_quantized_time = time.time()\n",
        "  if IS_HARDWARE_HANDLE_OUTPUT_LAYER_1:\n",
        "    feature_next_layer_float32 = feature_next_layer_tensor.to(dtype=torch.float64)\n",
        "    max_value = torch.max(feature_next_layer_float32)\n",
        "    min_value = torch.min(feature_next_layer_float32)\n",
        "    rounded_max_value = max_value.round().int()\n",
        "    bits_required_feature_next_layer = bits_required(min_value, rounded_max_value, False)\n",
        "    result_feature_next_layer_float32 = feature_next_layer_float32 // (2**(bits_required_feature_next_layer - 8))\n",
        "\n",
        "    input_layer = result_feature_next_layer_float32.to(dtype=torch.float32)\n",
        "    print(\"Check specific value\", feature_next_layer_float32[1941][10])\n",
        "    print(\"Check specific value // 4\", result_feature_next_layer_float32[1941][10])\n",
        "    print(\"Check input value // 4\", input_layer[1941][10])\n",
        "    print(\"Check by huynguyenn\", rounded_max_value, min_value, bits_required_feature_next_layer, torch.max(input_layer), torch.min(input_layer), input_layer.shape)\n",
        "  else:\n",
        "    feature_next_layer_quantized,_ = scale_tensor(feature_next_layer_tensor,\n",
        "                                              Configuration[\"BuildModel\"][\"scaleMin\"],\n",
        "                                              Configuration[\"BuildModel\"][\"scaleMax\"],\n",
        "                                              torch.int8)\n",
        "    input_layer = feature_next_layer_quantized.to(dtype=torch.float32)\n",
        "  end_quantized_time = time.time()\n",
        "  print(input_layer, input_layer.shape, data.x.shape[0])\n",
        "  print(f\"Time to quantized: {end_quantized_time - start_quantized_time:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOYWD27n-gM6"
      },
      "source": [
        "**Loading Model & Param quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVwBPD0tYPTM",
        "outputId": "ed0995ed-78dc-45f0-8772-6013b7bced8f"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "L2__SUJUn3Bl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "GlobalConfiguration = {\n",
        "    \"GAT\": {\n",
        "        \"hiddenChannel\": 16,\n",
        "        \"head\": 1\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"savePath\": \"model_params.pth\",\n",
        "        \"scaleMin\": -127,\n",
        "        \"scaleMax\": 127,\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"root\": \"data/Planetoid\",\n",
        "        \"name\": \"Cora\", # Cora, CiteSeer, PubMed\n",
        "        \"normalization\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "MappingModelParam = {\n",
        "    \"conv1.att_src\" : \"a_src_1\",\n",
        "    \"conv1.att_dst\" : \"a_dst_1\",\n",
        "    \"conv1.bias\" : \"b_1\",\n",
        "    \"conv1.lin.weight\" : \"w_1\",\n",
        "    \"conv2.att_src\" : \"a_src_2\",\n",
        "    \"conv2.att_dst\" : \"a_dst_2\",\n",
        "    \"conv2.bias\" : \"b_2\",\n",
        "    \"conv2.lin.weight\" : \"w_2\"\n",
        "}\n",
        "\n",
        "def tensor_to_list(tensor):\n",
        "    if not isinstance(tensor, torch.Tensor):\n",
        "        raise TypeError(\"Input must be a torch.Tensor\")\n",
        "\n",
        "    # Flatten the tensor and convert to a list\n",
        "    return tensor.flatten().tolist()\n",
        "\n",
        "def tensor_to_matrix(tensor):\n",
        "    return tensor.tolist()\n",
        "\n",
        "def list_or_matrix_to_tensor(data):\n",
        "    return torch.tensor(data)\n",
        "\n",
        "def format_number(value):\n",
        "    # Convert the number to a string\n",
        "    value_str = str(value)\n",
        "    # Check if the string ends with '.0' and remove it\n",
        "    if value_str.endswith('.0'):\n",
        "        value_str = value_str[:-2]\n",
        "    return value_str\n",
        "\n",
        "def int_to_n_bit_binary(number, n_bits):\n",
        "    # Handle two's complement for negative numbers\n",
        "    if number < 0:\n",
        "        number = (1 << n_bits) + number\n",
        "\n",
        "    # Convert the number to binary with zero-padding to n bits\n",
        "    binary_str = format(number, f'0{n_bits}b')\n",
        "    return binary_str\n",
        "\n",
        "def int_to_n_bit_binary_list(arr, n_bits):\n",
        "    binary_arrays = []\n",
        "    for num in arr:\n",
        "      binary_arrays.append(int_to_n_bit_binary(int(num), n_bits))\n",
        "    return binary_arrays\n",
        "\n",
        "def int_to_n_bit_binary_matrix(matrix, n_bits):\n",
        "    binary_matrix = []\n",
        "    for row in matrix:\n",
        "      binary_row = []\n",
        "      for col in row:\n",
        "        binary_row.append(int_to_n_bit_binary(int(col), n_bits))\n",
        "      binary_matrix.append(binary_row)\n",
        "    return binary_matrix\n",
        "\n",
        "def list_to_matrix(lst, rows, cols):\n",
        "    if len(lst) != rows * cols:\n",
        "        raise ValueError(\"List length must match rows * columns\")\n",
        "\n",
        "    return [lst[i * cols:(i + 1) * cols] for i in range(rows)]\n",
        "\n",
        "def matrix_to_list(matrix):\n",
        "    return [item for row in matrix for item in row]\n",
        "\n",
        "def quantized(tensor, scale_min, scale_max, to_dtype=torch.int8):\n",
        "    v_max = tensor.max() if tensor.max() != 0 else 1  # Avoid division by zero\n",
        "\n",
        "    # Scale the tensor\n",
        "    quantized_tensor = (tensor / v_max) * scale_max\n",
        "    quantized_tensor = quantized_tensor.clamp(scale_min, scale_max)\n",
        "    quantized_tensor = quantized_tensor.to(to_dtype)\n",
        "\n",
        "    # Define a function to scale back to the original range\n",
        "    def dequantized(quantized_tensor):\n",
        "        quantized_tensor = quantized_tensor.to(torch.float32)  # Ensure float for computation\n",
        "        return (quantized_tensor / scale_max) * v_max\n",
        "    return quantized_tensor, dequantized\n",
        "\n",
        "class DatasetLoaderV2:\n",
        "  def __init__(self,\n",
        "               root: str = GlobalConfiguration[\"dataset\"][\"root\"],\n",
        "               name: str = GlobalConfiguration[\"dataset\"][\"name\"],\n",
        "               normalize: int = GlobalConfiguration[\"dataset\"][\"normalization\"]):\n",
        "      self.root = root\n",
        "      self.name = name\n",
        "      self.normalize = normalize\n",
        "      self.dataset = self._load_dataset()\n",
        "\n",
        "  def _load_dataset(self):\n",
        "      transform = NormalizeFeatures() if self.normalize else None\n",
        "      return Planetoid(root=self.root, name=self.name, transform=transform)\n",
        "\n",
        "  def get_data(self, index: int = 0):\n",
        "      return self.dataset[index]\n",
        "\n",
        "  def get_dataset(self):\n",
        "      return self.dataset\n",
        "\n",
        "  def get_edges(self):\n",
        "      return self.dataset[0].edge_index\n",
        "\n",
        "  def get_isolated(self):\n",
        "      edges = self.get_edges()\n",
        "      edges_src = edges[0]\n",
        "      edges_dst = edges[1]\n",
        "      all_nodes = torch.unique(torch.cat([edges_src, edges_dst]))\n",
        "      total_nodes = self.get_data().x.shape[0]\n",
        "      isolated_nodes = [node for node in range(total_nodes) if node not in all_nodes]\n",
        "      isolated_map = {}\n",
        "      print(self.get_data().x.shape)\n",
        "      for node_idx in isolated_nodes:\n",
        "        isolated_map[node_idx] = self.get_data().x[node_idx]\n",
        "      return isolated_nodes, isolated_map\n",
        "\n",
        "class GATV2(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 data_loader,\n",
        "                 hidden_channels = GlobalConfiguration[\"GAT\"][\"hiddenChannel\"],\n",
        "                 heads = GlobalConfiguration[\"GAT\"][\"head\"]):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GATConv(data_loader.get_dataset().num_features, hidden_channels, heads, True)\n",
        "        self.conv2 = GATConv(heads * hidden_channels, data_loader.get_dataset().num_classes, 1, False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        p_default = 0.6\n",
        "        x = F.dropout(x, p=p_default, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=p_default, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class BuildModelV2():\n",
        "    def __init__(self, model, data_loader, save_path = GlobalConfiguration[\"model\"][\"savePath\"]):\n",
        "        self.model = model\n",
        "        self.save_path = save_path\n",
        "        self.data_loader = data_loader\n",
        "        self.load_model_params()\n",
        "\n",
        "    def load_model_params(self):\n",
        "        \"\"\"Load model parameters from the file if it exists.\"\"\"\n",
        "        if os.path.exists(self.save_path):\n",
        "            self.model.load_state_dict(torch.load(self.save_path))\n",
        "            print(f\"Model parameters loaded from {self.save_path}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"No saved model parameters found at {self.save_path}\")\n",
        "            return False\n",
        "\n",
        "    # Return:\n",
        "    # - Dictionary of parameters\n",
        "    # - Example: {'a_src_1': [...], 'a_dst_1': [...], 'a_1': [...], 'b_1': [...], 'a_src_2': [...], 'a_dst_2': [...], 'a_2': [...], 'b_2': [...]}\n",
        "    def get_model_params(self):\n",
        "        result = {}\n",
        "        param = self.model.state_dict();\n",
        "        for k, v in param.items():\n",
        "          quantized_v, _ = quantized(v,\n",
        "                                        GlobalConfiguration[\"model\"][\"scaleMin\"],\n",
        "                                        GlobalConfiguration[\"model\"][\"scaleMax\"],\n",
        "                                        torch.int8)\n",
        "          if quantized_v.ndim == 3:\n",
        "            quantized_v = quantized_v.reshape(quantized_v.shape[0], -1)\n",
        "          if k == \"conv1.lin.weight\" or k == \"conv2.lin.weight\":\n",
        "            quantized_v = quantized_v.t()\n",
        "          result[MappingModelParam[k]] = tensor_to_list(quantized_v)\n",
        "\n",
        "        result['a_1'] = result['a_src_1'] + result['a_dst_1']\n",
        "        result['a_2'] = result['a_src_2'] + result['a_dst_2']\n",
        "        return result\n",
        "\n",
        "    def get_raw_model(self):\n",
        "      param = self.model.state_dict()\n",
        "      for k, v in param.items():\n",
        "        quantized_v, _ = quantized(v,\n",
        "                                     GlobalConfiguration[\"model\"][\"scaleMin\"],\n",
        "                                     GlobalConfiguration[\"model\"][\"scaleMax\"],\n",
        "                                     torch.int8)\n",
        "        a[k] = quantized_v\n",
        "      return a\n",
        "\n",
        "    def test(self, visualization_2D = False, visualization_3D = False):\n",
        "        self.model.eval()\n",
        "        data = self.data_loader.get_data()\n",
        "        a = self.model.state_dict()\n",
        "        out = self.model(data.x, data.edge_index)\n",
        "        # pred = []\n",
        "        # for row in out:\n",
        "        #     max_value = row[0]\n",
        "        #     max_index = 0\n",
        "        #     for i in range(1, len(row)):\n",
        "        #         if row[i] > max_value:\n",
        "        #             max_value = row[i]\n",
        "        #             max_index = i\n",
        "        #     pred.append(max_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        # test_correct = []\n",
        "        # for i in range(len(data.y)):\n",
        "        #     if data.test_mask[i]:\n",
        "        #         is_correct = pred[i] == data.y[i]\n",
        "        #         test_correct.append(is_correct)\n",
        "        # test_acc = int(sum(test_correct)) / int(data.test_mask.sum())\n",
        "        test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
        "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
        "        return test_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7nKc1MAPEqdp"
      },
      "outputs": [],
      "source": [
        "# Params:\n",
        "  # - new_feature: List = [...]\n",
        "  # - gat_model = BuildModelV2\n",
        "# Return:\n",
        "  # - {'w_a': [1,2,3,4,...], 'h': [1,2,3,4,...]}\n",
        "def handle_new_feature(new_feature, gat_model, data_loader): #Todo: handle case isolated node\n",
        "  raw_data = data_loader.get_data()\n",
        "  raw_edge_data = raw_data.edge_index\n",
        "  raw_init_feature_data = raw_data.x\n",
        "  isolated_node, isolated_map = data_loader.get_isolated()\n",
        "  raw_model = gat_model.get_raw_model()\n",
        "\n",
        "  new_feature_matrix = []\n",
        "  curr_flat_index = 0\n",
        "\n",
        "  for row_idx in range(raw_init_feature_data.shape[0]):\n",
        "      if row_idx in isolated_map:\n",
        "          new_feature_matrix.extend(torch.matmul(isolated_map[row_idx] , raw_model[f\"conv1.lin.weight\"].to(dtype=torch.float32).t()).tolist())\n",
        "      else:\n",
        "          new_feature_matrix.extend(new_feature[curr_flat_index : curr_flat_index + GlobalConfiguration[\"GAT\"][\"hiddenChannel\"]])\n",
        "          curr_flat_index += GlobalConfiguration[\"GAT\"][\"hiddenChannel\"]\n",
        "\n",
        "  new_feature_matrix = list_to_matrix(new_feature_matrix, raw_init_feature_data.shape[0], GlobalConfiguration[\"GAT\"][\"hiddenChannel\"])\n",
        "  start_time = time.time()\n",
        "  new_feature_tensor_quantized,_ = quantized(list_or_matrix_to_tensor(new_feature_matrix),\n",
        "                                                  GlobalConfiguration[\"model\"][\"scaleMin\"],\n",
        "                                                  GlobalConfiguration[\"model\"][\"scaleMax\"],\n",
        "                                                  torch.int8)\n",
        "  new_feature_matrix_quantized = tensor_to_matrix(new_feature_tensor_quantized)\n",
        "  h_matrix = []\n",
        "  for src_idx in torch.unique(raw_edge_data[0]):\n",
        "      neighbors_idx_arr = raw_edge_data[1][raw_edge_data[0] == src_idx]\n",
        "      h_matrix.append(new_feature_matrix_quantized[src_idx])\n",
        "      for neighbor_idx in neighbors_idx_arr:\n",
        "        h_matrix.append(new_feature_matrix_quantized[neighbor_idx])\n",
        "  h_matrix_format_binary = [[int_to_n_bit_binary(int(format_number(value)), 8) for value in row] for row in h_matrix]\n",
        "  end_time = time.time()\n",
        "  print(f\"Time to run test: {end_time - start_time:.6f} seconds\")\n",
        "  return {\n",
        "      'weight': int_to_n_bit_binary_list(gat_model.get_model_params()['w_2'] + gat_model.get_model_params()['a_2'], 8),\n",
        "      'h_data': matrix_to_list(h_matrix_format_binary)\n",
        "  }\n",
        "\n",
        "def handle_classification(result_list, data_loader): #Todo: handle case isolated node\n",
        "  raw_dataset = data_loader.get_dataset()\n",
        "  raw_data = data_loader.get_data()\n",
        "  raw_init_feature_data = raw_data.x\n",
        "  isolated_node, isolated_map = data_loader.get_isolated()\n",
        "\n",
        "  curr_flat_index = 0\n",
        "  new_result_list = []\n",
        "  for row_idx in range(raw_init_feature_data.shape[0]):\n",
        "      if row_idx in isolated_map:\n",
        "          new_result_list.extend([0] * raw_dataset.num_classes)\n",
        "      else:\n",
        "          new_result_list.extend(result_list[curr_flat_index : curr_flat_index + raw_dataset.num_classes])\n",
        "          curr_flat_index += raw_dataset.num_classes\n",
        "\n",
        "  result_matrix = list_to_matrix(new_result_list, raw_init_feature_data.shape[0], raw_dataset.num_classes)\n",
        "  result_tensor = list_or_matrix_to_tensor(result_matrix)\n",
        "  result_classification = torch.argmax(result_tensor, dim=1)\n",
        "  print(\"Classification: \", result_classification)\n",
        "  print(\"Correct result\", raw_data.y)\n",
        "  test_indices = torch.where(raw_data.test_mask)[0]\n",
        "\n",
        "  match_count = 0;\n",
        "  for idx in range(raw_init_feature_data.shape[0]):\n",
        "    if idx in test_indices:\n",
        "      if idx in isolated_node:\n",
        "        match_count = match_count + 1\n",
        "      else:\n",
        "        if result_classification[idx].item() == raw_data.y[idx].item():\n",
        "          match_count = match_count + 1\n",
        "\n",
        "  # print(f\"Correct count: {match_count} / {raw_init_feature_data.shape[0]}, acc: {match_count / raw_init_feature_data.shape[0]}\")\n",
        "  print(f\"Correct count: {match_count} / {len(test_indices)}, acc: {match_count / len(test_indices)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6AyHehtJMP8",
        "outputId": "61c0c190-cff5-4925-832c-57a0194aacc2"
      },
      "outputs": [],
      "source": [
        "data_loader_instance = DatasetLoaderV2()\n",
        "gat_instance = GATV2(data_loader_instance)\n",
        "model_instance = BuildModelV2(gat_instance, data_loader_instance)\n",
        "\n",
        "\n",
        "# Prepare to receive output of layer 1\n",
        "print(test_new_feature, test_new_feature.shape)\n",
        "simulate_output_layer_1 = tensor_to_list(test_new_feature)\n",
        "simulate_output_layer_2 = tensor_to_list(test_output)\n",
        "handle_new_feature(simulate_output_layer_1, model_instance, data_loader_instance)['h_data']\n",
        "\n",
        "print()\n",
        "handle_classification(simulate_output_layer_2, data_loader_instance)\n",
        "# print(data_loader_instance.get_edges())\n",
        "# print(data_loader_instance.get_isolated())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqK_mg5_Zgsu",
        "outputId": "2d9e6a18-ccae-4533-c0d6-ef7340d08fc3"
      },
      "outputs": [],
      "source": [
        "start_test_time = time.time()\n",
        "test_acc = model_instance.test()\n",
        "end_test_time = time.time()\n",
        "print(f\"Initial Model Accuracy: {test_acc}, time to run test: {(end_test_time - start_test_time)*1000:.6f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcpvS_UQTQPv",
        "outputId": "816149d7-d42e-42ee-c325-af891ff89d31"
      },
      "outputs": [],
      "source": [
        "currentOption()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
