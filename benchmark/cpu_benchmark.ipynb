{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc4I546kjr81"
      },
      "source": [
        "# Capstone Project Test Running Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJi0UZxFl84P"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWph6ah8l_u7",
        "outputId": "30337206-45b3-4155-a524-caa7c110ad29"
      },
      "outputs": [],
      "source": [
        "# !pip install torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC0vKLn8llhG"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2IWiVGdIlnut"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import subprocess\n",
        "import platform\n",
        "import psutil\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj7wqzBJldE2"
      },
      "source": [
        "# Configuration & Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FlWQ8GY2lg2U"
      },
      "outputs": [],
      "source": [
        "GlobalConfiguration = {\n",
        "    \"GAT\": {\n",
        "        \"hiddenChannel\": 16,\n",
        "        \"head\": 1\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"savePath\": \"model_params.pth\",\n",
        "        \"scaleMin\": -127,\n",
        "        \"scaleMax\": 127,\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"root\": \"data/Planetoid\",\n",
        "        # \"name\": \"Cora\",\n",
        "        \"name\": \"CiteSeer\",\n",
        "        \"normalization\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "MappingModelParam = {\n",
        "    \"conv1.att_src\" : \"a_src_1\",\n",
        "    \"conv1.att_dst\" : \"a_dst_1\",\n",
        "    \"conv1.bias\" : \"b_1\",\n",
        "    \"conv1.lin.weight\" : \"w_1\",\n",
        "    \"conv2.att_src\" : \"a_src_2\",\n",
        "    \"conv2.att_dst\" : \"a_dst_2\",\n",
        "    \"conv2.bias\" : \"b_2\",\n",
        "    \"conv2.lin.weight\" : \"w_2\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sPB26pnkOnq"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lfo0vJ6wj7j7"
      },
      "outputs": [],
      "source": [
        "def tensor_to_list(tensor):\n",
        "    if not isinstance(tensor, torch.Tensor):\n",
        "        raise TypeError(\"Input must be a torch.Tensor\")\n",
        "\n",
        "    # Flatten the tensor and convert to a list\n",
        "    return tensor.flatten().tolist()\n",
        "\n",
        "def quantized(tensor, scale_min, scale_max, to_dtype=torch.int8):\n",
        "    v_max = tensor.max() if tensor.max() != 0 else 1  # Avoid division by zero\n",
        "\n",
        "    # Scale the tensor\n",
        "    quantized_tensor = (tensor / v_max) * scale_max\n",
        "    quantized_tensor = quantized_tensor.clamp(scale_min, scale_max)\n",
        "    quantized_tensor = quantized_tensor.to(to_dtype)\n",
        "\n",
        "    # Define a function to scale back to the original range\n",
        "    def dequantized(quantized_tensor):\n",
        "        quantized_tensor = quantized_tensor.to(torch.float32)  # Ensure float for computation\n",
        "        return (quantized_tensor / scale_max) * v_max\n",
        "    return quantized_tensor, dequantized\n",
        "\n",
        "def currentOption():\n",
        "  if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    try:\n",
        "        output = subprocess.check_output(\n",
        "            [\"nvidia-smi\", \"--query-gpu=clocks.gr,clocks.sm,clocks.mem\", \"--format=csv,noheader,nounits\"],\n",
        "            encoding='utf-8'\n",
        "        )\n",
        "        gr, sm, mem = output.strip().split(', ')\n",
        "        print(f\"GPU Graphics Clock: {gr} MHz\")\n",
        "        print(f\"GPU SM Clock: {sm} MHz\")\n",
        "        print(f\"GPU Memory Clock: {mem} MHz\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not fetch GPU frequency. Make sure 'nvidia-smi' is installed.\")\n",
        "  else:\n",
        "      print(\"Running on CPU.\")\n",
        "\n",
        "  # CPU Information\n",
        "  print(f\"Processor: {platform.processor()}\")\n",
        "  print(f\"CPU Count: {os.cpu_count()}\")\n",
        "\n",
        "  # RAM Information\n",
        "  virtual_memory = psutil.virtual_memory()\n",
        "  print(f\"Total RAM: {virtual_memory.total / 1e9:.2f} GB\")\n",
        "  print(f\"Available RAM: {virtual_memory.available / 1e9:.2f} GB\")\n",
        "\n",
        "  #Frequency\n",
        "  cpu_freq = psutil.cpu_freq()\n",
        "  if cpu_freq:\n",
        "      print(f\"CPU Frequency: {cpu_freq.current:.2f} MHz (Max: {cpu_freq.max:.2f} MHz)\")\n",
        "\n",
        "\n",
        "  # Disk Information\n",
        "  disk_usage = psutil.disk_usage('/')\n",
        "  print(f\"Total Disk Space: {disk_usage.total / 1e9:.2f} GB\")\n",
        "  print(f\"Used Disk Space: {disk_usage.used / 1e9:.2f} GB\")\n",
        "  print(f\"Free Disk Space: {disk_usage.free / 1e9:.2f} GB\")\n",
        "\n",
        "  # Operating System Information\n",
        "  print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
        "  print(f\"Python Version: {platform.python_version()}\")\n",
        "\n",
        "def benchmark_test(model_instance, runs=100, verbose=True):\n",
        "  times = []\n",
        "  last_accuracy = None\n",
        "\n",
        "  for _ in range(runs):\n",
        "      start_time = time.time()\n",
        "      last_accuracy = model_instance.test()\n",
        "      end_time = time.time()\n",
        "      times.append(end_time - start_time)\n",
        "\n",
        "  mean_time = np.mean(times)\n",
        "\n",
        "  if verbose:\n",
        "      print()\n",
        "      print(f\"‚úÖ Benchmark complete: {runs} runs\")\n",
        "      print(f\"‚è±Ô∏è Mean test time: {round(mean_time*1000)} ms\")\n",
        "      print(f\"üéØ Last test accuracy: {last_accuracy*100}%\")\n",
        "\n",
        "  return mean_time, last_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffGgXuQZkeDa"
      },
      "source": [
        "# Dataset Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KWLAB4tBkmET"
      },
      "outputs": [],
      "source": [
        "class DatasetLoaderV2:\n",
        "  def __init__(self,\n",
        "               root: str = GlobalConfiguration[\"dataset\"][\"root\"],\n",
        "               name: str = GlobalConfiguration[\"dataset\"][\"name\"],\n",
        "               normalize: int = GlobalConfiguration[\"dataset\"][\"normalization\"]):\n",
        "      self.root = root\n",
        "      self.name = name\n",
        "      self.normalize = normalize\n",
        "      self.dataset = self._load_dataset()\n",
        "\n",
        "  def _load_dataset(self):\n",
        "      transform = NormalizeFeatures() if self.normalize else None\n",
        "      return Planetoid(root=self.root, name=self.name, transform=transform)\n",
        "\n",
        "  def get_data(self, index: int = 0):\n",
        "      return self.dataset[index]\n",
        "\n",
        "  def get_dataset(self):\n",
        "      return self.dataset\n",
        "\n",
        "  def get_edges(self):\n",
        "      return self.dataset[0].edge_index\n",
        "\n",
        "  def get_isolated(self):\n",
        "      edges = self.get_edges()\n",
        "      edges_src = edges[0]\n",
        "      edges_dst = edges[1]\n",
        "      all_nodes = torch.unique(torch.cat([edges_src, edges_dst]))\n",
        "      total_nodes = self.get_data().x.shape[0]\n",
        "      isolated_nodes = [node for node in range(total_nodes) if node not in all_nodes]\n",
        "      isolated_map = {}\n",
        "      print(self.get_data().x.shape)\n",
        "      for node_idx in isolated_nodes:\n",
        "        isolated_map[node_idx] = self.get_data().x[node_idx]\n",
        "      return isolated_nodes, isolated_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6yB2x13lCq1"
      },
      "source": [
        "# GAT Algorithm Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XmaKxEhJqPto"
      },
      "outputs": [],
      "source": [
        "class GATV2(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 data_loader,\n",
        "                 hidden_channels = GlobalConfiguration[\"GAT\"][\"hiddenChannel\"],\n",
        "                 heads = GlobalConfiguration[\"GAT\"][\"head\"]):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GATConv(data_loader.get_dataset().num_features, hidden_channels, heads, True)\n",
        "        self.conv2 = GATConv(heads * hidden_channels, data_loader.get_dataset().num_classes, 1, False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        p_default = 0.6\n",
        "        x = F.dropout(x, p=p_default, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=p_default, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ac770sHlS-X"
      },
      "source": [
        "# Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DxgpPJaNlWC_"
      },
      "outputs": [],
      "source": [
        "class BuildModelV2():\n",
        "    def __init__(self, model, data_loader, save_path = GlobalConfiguration[\"model\"][\"savePath\"]):\n",
        "        self.model = model\n",
        "        self.save_path = save_path\n",
        "        self.data_loader = data_loader\n",
        "        self.load_model_params()\n",
        "\n",
        "    def load_model_params(self):\n",
        "        \"\"\"Load model parameters from the file if it exists.\"\"\"\n",
        "        basePath = \"model\"\n",
        "        fullPath = os.path.join(basePath, GlobalConfiguration[\"dataset\"][\"name\"], self.save_path)\n",
        "        if os.path.exists(fullPath):\n",
        "            self.model.load_state_dict(torch.load(fullPath))\n",
        "            print(f\"Model parameters loaded from {fullPath}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"No saved model parameters found at {fullPath}\")\n",
        "            return False\n",
        "\n",
        "    # Return:\n",
        "    # - Dictionary of parameters\n",
        "    # - Example: {'a_src_1': [...], 'a_dst_1': [...], 'a_1': [...], 'b_1': [...], 'a_src_2': [...], 'a_dst_2': [...], 'a_2': [...], 'b_2': [...]}\n",
        "    def get_model_params(self):\n",
        "        result = {}\n",
        "        param = self.model.state_dict();\n",
        "        for k, v in param.items():\n",
        "          quantized_v, _ = quantized(v,\n",
        "                                        GlobalConfiguration[\"model\"][\"scaleMin\"],\n",
        "                                        GlobalConfiguration[\"model\"][\"scaleMax\"],\n",
        "                                        torch.int8)\n",
        "          if quantized_v.ndim == 3:\n",
        "            quantized_v = quantized_v.reshape(quantized_v.shape[0], -1)\n",
        "          if k == \"conv1.lin.weight\" or k == \"conv2.lin.weight\":\n",
        "            quantized_v = quantized_v.t()\n",
        "          result[MappingModelParam[k]] = tensor_to_list(quantized_v)\n",
        "\n",
        "        result['a_1'] = result['a_src_1'] + result['a_dst_1']\n",
        "        result['a_2'] = result['a_src_2'] + result['a_dst_2']\n",
        "        return result\n",
        "\n",
        "    def test(self, visualization_2D=False, visualization_3D=False):\n",
        "        self.model.eval()\n",
        "        data = self.data_loader.get_data()\n",
        "\n",
        "        # T√≠nh to√°n ƒë·∫ßu ra c·ªßa m√¥ h√¨nh\n",
        "        out = self.model(data.x, data.edge_index)\n",
        "\n",
        "        # S·ª≠ d·ª•ng torch.argmax ƒë·ªÉ l·∫•y ch·ªâ m·ª•c c√≥ gi√° tr·ªã l·ªõn nh·∫•t trong m·ªói d√≤ng c·ªßa tensor out\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "\n",
        "        # S·ª≠ d·ª•ng test_mask ƒë·ªÉ ch·ªâ l·∫•y c√°c ph·∫ßn t·ª≠ c·∫ßn so s√°nh\n",
        "        correct_predictions = (pred == data.y) & data.test_mask\n",
        "\n",
        "        # T√≠nh to√°n ƒë·ªô ch√≠nh x√°c\n",
        "        test_acc = correct_predictions.sum().item() / data.test_mask.sum().item()\n",
        "\n",
        "        return test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnbDIxJomShj"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWXQHx-ooQC7",
        "outputId": "1018db52-ae1d-4c70-b22c-7624f458a142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on CPU.\n",
            "Processor: AMD64 Family 25 Model 68 Stepping 1, AuthenticAMD\n",
            "CPU Count: 16\n",
            "Total RAM: 29.73 GB\n",
            "Available RAM: 11.97 GB\n",
            "CPU Frequency: 3301.00 MHz (Max: 3301.00 MHz)\n",
            "Total Disk Space: 701.16 GB\n",
            "Used Disk Space: 480.31 GB\n",
            "Free Disk Space: 220.85 GB\n",
            "Operating System: Windows 10\n",
            "Python Version: 3.11.4\n"
          ]
        }
      ],
      "source": [
        "currentOption()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G5_-pjImUIs",
        "outputId": "1b58588c-8d95-423f-d3e3-6d00da9e7bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters loaded from model\\CiteSeer\\model_params.pth\n",
            "\n",
            "‚úÖ Benchmark complete: 100 runs\n",
            "‚è±Ô∏è Mean test time: 19 ms\n",
            "üéØ Last test accuracy: 71.6%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_loader_instance = DatasetLoaderV2()\n",
        "gat_instance = GATV2(data_loader_instance)\n",
        "model_instance = BuildModelV2(gat_instance, data_loader_instance)\n",
        "\n",
        "benchmark_test(model_instance, runs=100)\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
